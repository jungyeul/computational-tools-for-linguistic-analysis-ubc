{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LING 242 Python Lecture 4: Corpus Statistics\n",
    "\n",
    "* Advanced word counting\n",
    "* Advanced sorting\n",
    "* Simple statistics\n",
    "* Comparing corpora\n",
    "* T/F questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get counts of words from the Brown again, using a Counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "\n",
    "counts_wo_lower = Counter(brown.words())\n",
    "counts = Counter(word.lower() for word in brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56057\n",
      "49815\n"
     ]
    }
   ],
   "source": [
    "print(len(counts_wo_lower))\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often you need to normalize the counts in a dictionary, to create a word probability distribution. You can keep a running total or, more conveniently, just [sum](https://docs.python.org/3/library/functions.html#sum) the values of your count dict (you can also use `np.sum`). If you've done this right, your new values should sum to 1 (or close enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = sum(counts.values())\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69971\n",
      "0.06025790739171472\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum(counts.values())\n",
    "\n",
    "probs = {}\n",
    "\n",
    "for word in counts:\n",
    "    probs[word] = counts[word]/total_tokens\n",
    "    \n",
    "print(counts[\"the\"]) # 62713 by counts_wo_lower\n",
    "print(probs[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999991869"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(probs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why would word probability be more useful than raw counts when we are trying to use statistics to characterize corpora?\n",
    "\n",
    "Answer: <span style=\"background-color:black;\">Corpora can be of vastly different sizes, and those sizes will have a direct effect on word count. For example, one corpus might have 1 mention of the word _dog_ and another might have 100. This might indicate the second corpus talks a lot more about dogs, or it might just indicate that the second one is 100X as large</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For easy interpretability and to avoid low numbers, one often multiples these normalized word probabilities by some large number like 1000, at which point the resulting number can be understood as X occurrences per 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018429338128405983\n",
      "-8.598981606662296\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "log_probs = {}\n",
    "\n",
    "for word in probs:\n",
    "    log_probs[word] = math.log(probs[word])\n",
    "\n",
    "\n",
    "print(probs[\"university\"])\n",
    "print(log_probs[\"university\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18429338128405984"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_1k = {}\n",
    "\n",
    "for word in probs:\n",
    "    probs_1k[word] = probs[word] * 1000\n",
    "    \n",
    "probs_1k[\"university\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another common use case involving counts is removing words with high or low counts, which are often uninteresting or statistically unreliable. This is tricky, because in Python you can't delete from something you're iterating over! Unless you're very worried about lack of memory, usually easier to just create a new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49815\n",
      "12416\n"
     ]
    }
   ],
   "source": [
    "new_counts = {word:count for word,count in counts.items() if 5 < count}\n",
    "print(len(counts))\n",
    "print(len(new_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12406\n"
     ]
    }
   ],
   "source": [
    "new_counts = {word:count for word,count in counts.items() if 5 < count < 10000}\n",
    "print(len(new_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you're just interested in the highest (or lowest) count item, it is easy enough just to iterate over the dictionary once and remember the top scoring item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "69971\n"
     ]
    }
   ],
   "source": [
    "highest_count_word = \"\"# None\n",
    "highest_count = 0\n",
    "\n",
    "for word, count in counts.items():\n",
    "    if count > highest_count:\n",
    "        highest_count_word = word\n",
    "        highest_count = count\n",
    "\n",
    "print(highest_count_word)\n",
    "print(highest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69971"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None is just None\n"
     ]
    }
   ],
   "source": [
    "highest_count_word = None\n",
    "print(highest_count_word)\n",
    "\n",
    "x = None\n",
    "\n",
    "if x:  # if x is True:\n",
    "    print(\"True?\")\n",
    "elif x is False:\n",
    "    print (\"False?\")\n",
    "else:\n",
    "    print(\"None is just None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if you're using a Counter object, the [most_common](https://docs.python.org/3/library/collections.html#collections.Counter.most_common) method is often handy. Counters have a few other neat options, for instance they can be added and subtracted (though, unlike `update`, this creates a new Counter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 74735), (',', 63219), ('.', 53174), ('of', 38737), ('and', 30409), ('to', 28340), ('a', 25183), ('in', 23106), ('that', 11442), ('is', 10781)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "treebank_counts = Counter(word.lower() for word in treebank.words()) # Counter(treebank.words())\n",
    "both_counts = counts + treebank_counts\n",
    "print(both_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond that, you'll want to do some sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you've already seen, simple sorting of a list of objects in Python is fairly straightward. Use the [sort](https://docs.python.org/3/library/stdtypes.html#list.sort) method to sort in place, or [sorted](https://docs.python.org/3/library/functions.html#sorted) to create a new sorted list. Result is order from smallest to largest, use reverse keyword to reverse the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nums = [3, 6, -4, 23, 0.5, 202, -24592, 3482]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-24592, -4, 0.5, 3, 6, 23, 202, 3482]\n",
      "[3482, 202, 23, 6, 3, 0.5, -4, -24592]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(nums))\n",
    "print(sorted(nums,reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, -4, 23, 0.5, 202, -24592, 3482]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3482, 202, 23, 6, 3, 0.5, -4, -24592]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-24592, -4, 0.5, 3, 6, 23, 202, 3482]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.sort()\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3482, 202, 23, 6, 3, 0.5, -4, -24592]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2 = sorted(nums,reverse=True)\n",
    "nums2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that strings are generally sorted alphabetically, but once you get outside of a-z things can get unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['110', '12', '2', 'Aardvark', 'Zebra', 'aardvark', 'zebra']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = [\"aardvark\", \"Aardvark\", \"Zebra\", \"zebra\", \"12\", \"110\", \"2\"]\n",
    "\n",
    "sorted(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Often, though, you have a statistic associated with a group of objects (words, documents, corpora, etc.), and want to sort the objects based on the statistic. One strategy is to create a list of tuples where the statistic is the first element of the tuple, since sort operates on the first element of each tuple first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(69, 'fox'), (327, 'quick'), (539, 'brown'), (87925, 'the')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {\"the\":87925, \"quick\":327, \"brown\":539, \"fox\":69}\n",
    "sorted((count,word) for word, count in counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brown', 539), ('fox', 69), ('quick', 327), ('the', 87925)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted((word,count) for word, count in counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'quick', 'brown', 'fox'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An similarly compact, elegant way is to use the `key` keyword for *sort/sorting* function which allows you to specify a function which will define the value to sort a given iterable. The typical way to specify the function for this is to use a [lambda expression](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions). One clear advantage of the `key` approach is that you just get the sorted list without having to deal with extracting what you need from tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'fox', 'quick', 'the']\n",
      "[69, 327, 539, 87925]\n",
      "['brown', 'fox', 'quick', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(counts.keys()))\n",
    "print(sorted(counts.values()))\n",
    "print(sorted(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 87925, 'quick': 327, 'brown': 539, 'fox': 69}\n",
      "['brown', 'fox', 'quick', 'the']\n",
      "['the', 'fox', 'quick', 'brown']\n",
      "['fox', 'quick', 'brown', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(counts)\n",
    "\n",
    "sorted_words = sorted(counts.keys(), key=lambda x: x)\n",
    "print(sorted_words)\n",
    "sorted_words = sorted(counts.keys(), key=lambda x: len(x))\n",
    "print(sorted_words)\n",
    "sorted_words = sorted(counts.keys(), key=lambda x: counts[x])\n",
    "print(sorted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once you have a sorted list, you can use slicing to get what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'the']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_words[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use sorting with lambdas to get the 50 longest and shortest word types in the Penn Treebank corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', '9', \"'\", '@', '*']\n",
      "['collective-bargaining', 'Bridgestone\\\\/Firestone', 'computer-system-design', 'Macmillan\\\\/McGraw-Hill', 'marketing-communications']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "sorted_by_length = sorted(set(treebank.words()), key=lambda x: len(x))\n",
    "\n",
    "print(sorted_by_length[:5])\n",
    "print(sorted_by_length[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember that are two other python built-in functions, [min](https://docs.python.org/3/library/functions.html#min) and [max](https://docs.python.org/3/library/functions.html#min) which get the minimum and maximum values. Like sort/sorted, they have a *key* keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marketing-communications'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(set(treebank.words()), key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you've seen elsewhere, there are more ways to sort when you are using numpy arrays or pandas dataframes, but that is beyond our scope here! When you're dealing with relatively simple situations (like word counts) and there is little benefit to be gained from vectorization, you probably don't want the overhead of converting to these formats: pure Python is easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The easiest sorts of corpus statistics to calculate are averages: e.g. average word length, average sentence length, average words per text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_simple_stats(corpus):\n",
    "    num_chars = sum([len(word) for word in corpus.words()])\n",
    "    num_words = len(corpus.words())\n",
    "    num_sents = len(corpus.sents())\n",
    "    num_texts = len(corpus.fileids())\n",
    "    print(\"average word length\\t =\", num_chars/num_words)\n",
    "    print(\"average sentence length\\t =\", num_words/num_sents)\n",
    "    print(\"average text length\\t =\", num_words/num_texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average word length\t = 4.276538246904905\n",
      "average sentence length\t = 20.250994070456922\n",
      "average text length\t = 2322.384\n"
     ]
    }
   ],
   "source": [
    "get_simple_stats(brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One popular statistic for individual texts that reflects how varied the vocabulary used in the text is the type-token ratio (TTR), i.e.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac {\\text{No. of word types}}{\\text{No. of word token}}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that when you are using it for comparison, you generally need to fix the number of tokens for the texts you're comparing, since TTR can be quite different for different numbers of tokens. As we've seen already seen, sets are an easy way to get the number of types, though normally you'll want to lower case first.\n",
    "\n",
    "Let's generalize this into a function for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def type_token_ratio(words, *argv):\n",
    "    '''\n",
    "    calculate type-token ratio from the corpus of word tokens (list of strings) using the first\n",
    "    num_words tokens\n",
    "    '''\n",
    "    \n",
    "    num_words = argv[0] if argv else len(words) \n",
    "    types = set([word.lower() for word in words[:num_words]])\n",
    "    \n",
    "    return len(types)/num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04289988219002542"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_token_ratio(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.417"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_token_ratio(brown.words(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13082"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_token_ratio(brown.words(), 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The relative quantity of the main _open-class_ part-of-speech can reflect the nature of a particular corpus. For example, narrative texts (like stories) have more verbs, and informational texts (like technical reports) tend to have more nouns. If we have POS tags, this is easy to calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23521002555994186\n",
      "0.09976127978835542\n",
      "0.06200008267366637\n"
     ]
    }
   ],
   "source": [
    "noun_count = 0\n",
    "verb_count = 0\n",
    "adj_count = 0\n",
    "\n",
    "for word, pos in brown.tagged_words():\n",
    "    if pos[0] == \"N\":\n",
    "        noun_count += 1\n",
    "    if pos[0] == \"V\":\n",
    "        verb_count += 1\n",
    "    if pos[0] == \"J\":\n",
    "        adj_count += 1\n",
    "\n",
    "print(noun_count/len(brown.words()))\n",
    "print(verb_count/len(brown.words()))\n",
    "print(adj_count/len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One popular POS summary statistic is lexical density, which can also be calculated using a POS-tagged corpus. It is the ratio of open-class words (nouns, verbs, adjectives, adverbs) to all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43480664696277616\n"
     ]
    }
   ],
   "source": [
    "open_class_prefix = {\"N\", \"V\", \"J\", \"R\"}\n",
    "open_class_total = 0\n",
    "for word, pos in brown.tagged_words():\n",
    "    if pos[0] in open_class_prefix:\n",
    "        open_class_total += 1\n",
    "print(open_class_total/len(brown.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Of course, any word or POS sequence or otherwise easy identified linguistic property may be considered a potential statistic. For example, the code below counts English split infinitives (i.e. TO + RB + V) appear per 1000 words in the Brown (and prints them out). These particular construction was considered ungrammatical English by for hundreds of years, but it is actually quite common (e.g. in the opening to the _Star Trek_ TV show: _to boldly go where no man/one has gone before_)\n",
    "\n",
    "<img src=\"https://i.imgur.com/P8t4dDL.png\" style=\"width:300px;height:200px;\">\n",
    "(From OneirosTheWriter at sufficientvelocity.com) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TBBT S10E24: _The Long Distance Dissonance_**\n",
    "\n",
    "- Sheldon: I'm sorry you had to go through that.\n",
    "- Amy: In fact, that's when I started to really miss you.\n",
    "- Sheldon: You know you just split an infinitive.\n",
    "- Amy: Did I? Are you gonna teach me a lesson?\n",
    "- Sheldon: I am. It is naughty to put an adverb between the word \"to\" and the verb stem.\n",
    "- Amy: What are you gonna do about it?\n",
    "- Sheldon: I'm going to admonish you.\n",
    "- Amy: Vigorously?\n",
    "- Sheldon: That's the only kind of admonishing I do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to formally request',\n",
       " 'to completely bypass',\n",
       " 'to merely go',\n",
       " 'to properly express',\n",
       " 'to properly display']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_infinitives = 0\n",
    "split_infinitives_list = []\n",
    "for sent in brown.tagged_sents():\n",
    "    for i in range(len(sent) - 2):\n",
    "        if sent[i][1] == \"TO\" and sent[i+1][1][:2] == \"RB\" and sent[i+2][1][0] == \"V\":\n",
    "            split_infinitives_list.append(sent[i][0] + \" \" + sent[i+1][0] + \" \" + sent[i+2][0])\n",
    "            split_infinitives += 1\n",
    "            \n",
    "# print(\"Frequency per 100000 words\")     \n",
    "# print(100000*split_infinitives/len(brown.words())) # 2.3251968666680445\n",
    "split_infinitives_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus linguists use these methods to investigate how lingustic patterns of interest are being used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We count and sort words and derive corpus statistics primarily so we can identify differences between texts or corpora. Let's calculate and compare some basic stats for some corpora in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ordered_bar_from_dict(py_dict, title):\n",
    "    '''create a bar chart from values in py_dict, ordered from smallest to largest and labeled with keys'''\n",
    "    labels = sorted(py_dict.keys(),key=lambda x: py_dict[x])\n",
    "    y_pos = np.arange(len(labels))\n",
    "    values = sorted(py_dict.values())\n",
    "\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5,color=list('rgbkym'))\n",
    "    plt.xticks(y_pos, labels,rotation=45)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(words):\n",
    "    '''calculate the average length of the provided words'''\n",
    "    total_words = 0\n",
    "    total_chars = 0\n",
    "    for word in words:\n",
    "        total_words += 1\n",
    "        total_chars += len(word)\n",
    "    return total_chars/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "corpora = [\"treebank\", \"gutenberg\", \"reuters\", \"switchboard\",\"webtext\", \"movie_reviews\"]\n",
    "\n",
    "exec(\"from nltk.corpus import \" + \", \".join(corpora))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n9/3r6hrp015t9d1n6m8l36t0yh0000gn/T/ipykernel_38823/2033851823.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"treebank\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreebank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "corpus = \"treebank\"\n",
    "print(len(treebank.words()))\n",
    "print(len(corpus.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'treebank': 4.406154396281139,\n",
       " 'gutenberg': 3.618868231123358,\n",
       " 'reuters': 4.0016898124877605,\n",
       " 'switchboard': 3.240325152188617,\n",
       " 'webtext': 3.552701691061747,\n",
       " 'movie_reviews': 3.9314442297735854}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_word_lengths = {}\n",
    "for corpus in corpora:\n",
    "    exec(\"words = \" +corpus + \".words()\")\n",
    "    avg_word_lengths[corpus] = average_word_length(words)\n",
    "\n",
    "avg_word_lengths\n",
    "# ordered_bar_from_dict(avg_word_lengths,\"Average word lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'treebank': 0.11310540744566729,\n",
       " 'gutenberg': 0.016149980946844555,\n",
       " 'reuters': 0.01805914459925353,\n",
       " 'switchboard': 0.052855348342835055,\n",
       " 'webtext': 0.043893500162577856,\n",
       " 'movie_reviews': 0.02510891389173012}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttrs = {}\n",
    "for corpus in corpora:\n",
    "    exec(\"words = \" +corpus + \".words()\")\n",
    "    ttrs[corpus] = type_token_ratio(words)\n",
    "\n",
    "ttrs\n",
    "# ordered_bar_from_dict(ttrs,\"Type token ratios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a little bit about what we see here, and how it reflects the differences between corpora..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to identify individual words (or other linguistic features) that are particularly important in one corpus relative to another.  We are going to do this for just two very different corpora, gutenberg and webtext. Let's go back to our word (unigram) probabilities. First, we build them for each corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_probs(words):\n",
    "    '''get unigram probabilities for the words in a corpus'''\n",
    "    counts = Counter(word.lower() for word in words)\n",
    "    total = sum(counts.values())\n",
    "    return {word:count/total for word,count in counts.items()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_probs = get_unigram_probs(webtext.words())\n",
    "guten_probs = get_unigram_probs(gutenberg.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two natural, relatively direct ways to use these probabilities to identify words that are characteristic of the two corpora. One is to rank the words by the _difference_ between probabilities, and the other is to rank the words by the _ratio_ of the probabilities. Both will work, but they lead to different results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_probs(prob1, prob2):\n",
    "    '''given two probability dictionaries, create a dictionary has the difference of probabilities (prob1 - prob2)\n",
    "    for words that appear in either dictionary'''\n",
    "    all_words = set(prob1.keys())\n",
    "    all_words.update(prob2.keys())\n",
    "    return {word:prob1.get(word,0) - prob2.get(word,0) for word in all_words}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict = subtract_probs(guten_probs,web_probs)\n",
    "sub_sorted_words = sorted(sub_dict.keys(),key=lambda x: sub_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", ':', '.', '#', 'i', 'you', '?', 't', '!', 'girl', 'guy', 'on', '-', '1', 's', '...', '2', ']', '[', 'like', 'don', 'm', 'a', 'yeah', 'page', 'firefox', 'when', 'can', 'woman', 'just', 're', 'get', 'chick', 'does', 'new', 'no', '(', ')', 'window', 'bookmarks', 'open', 'doesn', 'teen', 'firebird', 'cell', 'know', 'is', 'menu', 'tab', 'bar']\n"
     ]
    }
   ],
   "source": [
    "print(sub_sorted_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['men', 'from', 'house', 'hath', 'israel', 'before', 'king', 'will', 'at', '--', 'came', 'she', 'god', 'upon', 'but', ',\"', 'ye', '.\"', 'with', 'thee', 'were', 'in', 'thy', 'by', 'to', 'thou', 'their', 'for', 'all', 'they', 'be', 'her', 'which', 'had', 'said', 'them', 'lord', 'unto', 'as', 'him', 'that', 'was', 'shall', 'he', 'his', ';', 'of', 'and', 'the', ',']\n"
     ]
    }
   ],
   "source": [
    "print(sub_sorted_words[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we subtract probabilities, we tend to get a lot of very common words (since these words have high probabilities to begin with)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the ratio method. One problem with the ratio is the potential for divide by zero errors, hence we will only look at the shared vocabulary:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_probs(prob1, prob2):\n",
    "    '''given two probability dictionaries, create a dictionary has the ratios of probabilities (prob1/prob2)\n",
    "    for each word included in both'''\n",
    "    all_words = set(prob1.keys()).intersection(prob2.keys())\n",
    "    return {word:prob1[word]/prob2[word] for word in all_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_dict = divide_probs(guten_probs,web_probs)\n",
    "div_sorted_words = sorted(div_dict.keys(),key=lambda x: div_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guy', '0', 'clicking', 'tourist', 'folder', '+', 'download', 'password', 'dad', 'option', 'turner', 'install', 'phoenix', 'default', 'bitch', 'user', 'html', '?...', 'location', 'os', 'status', 'auto', 'context', 'bug', 'focus', 'settings', 'click', 'anymore', 'extension', 'disable', 'installed', 'pussy', 'delete', 'site', '***', 'jewish', 'cashier', 'cop', 'cute', 'loading', 'cancel', 'program', '<', '>', 'fails', 'font', 'yo', 'data', 'blocking', 'switch']\n"
     ]
    }
   ],
   "source": [
    "print(div_sorted_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21', 'wrath', 'sight', 'ark', '19', 'offering', 'sin', 'abraham', ',\"', 'offerings', 'cried', 'pharaoh', 'grace', 'among', '22', 'princes', 'elliot', 'receive', 'stood', 'angel', 'wilderness', '27', 'solomon', 'spirits', 'shalt', 'mercy', 'aaron', 'shall', 'whom', 'israel', '14', 'praise', 'wherefore', '29', 'spoken', 'replied', 'commanded', 'behold', 'thou', 'sons', 'spake', 'thus', 'therefore', 'mrs', 'thine', 'thy', 'thee', 'hast', 'whale', 'ye']\n"
     ]
    }
   ],
   "source": [
    "print(div_sorted_words[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words found using ratios are more distinctive, but there is also noise, because it includes some very low probability types.\n",
    "\n",
    "Question: What's a possible quick solution to this problem of noise? (Something we've already talked about in this lecture)\n",
    "\n",
    "Answer: <span style=\"background-color:black;\"> We could remove words with low counts in one or both corpora </span>\n",
    "\n",
    "As it happens, there are more sophisticated statistical methods that find a better balance between these two extremes (being biased neither towards extremely common nor extremely rare words), but we'll stop here for now... "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
