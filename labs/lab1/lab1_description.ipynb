{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a06a5b5",
   "metadata": {},
   "source": [
    "# Lab 1: Preprocessing\n",
    "\n",
    "Lab description written by Kiki Wang (LING242 - 2021W1)\n",
    "\n",
    "## 0. prepraring data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e6802",
   "metadata": {},
   "source": [
    "Text preprocessing is a crucial step for natural language processing (NLP) tasks as it transform texts into forms that could be recognized by various computational models or machine learning algorithms.\n",
    "\n",
    "The objective of this lab is to perform the following preprocessing tasks:\n",
    "(1) Tokenization: splitting strings of texts into smaller units with distinct meanings, which are known as \"tokens\";\n",
    "(2) Sentence Boundary Disambiguation/Detection (SBD): processing text at the sentence level which entails identifying start and end of sentences.\n",
    "\n",
    "In this lab, we employ 2 tokenization scripts, namely Penn treebank-style tokenization script (`tokenizer.sed`) and Corpus preparation from Moses (`tokenizer.perl`). For SBD, we use the pretrained model Splitta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc7c629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-01 00:08:06--  https://raw.githubusercontent.com/jungyeul/computational-tools-for-linguistic-analysis-ubc/main/labs/data/en_ewt-ud-dev-alt.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2021-10-01 00:08:06 ERROR 404: Not Found.\n",
      "\n",
      "--2021-10-01 00:08:06--  https://raw.githubusercontent.com/jungyeul/computational-tools-for-linguistic-analysis-ubc/main/labs/data/en_ewt-ud-dev.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 126870 (124K) [text/plain]\n",
      "Saving to: ‘en_ewt-ud-dev.txt’\n",
      "\n",
      "en_ewt-ud-dev.txt   100%[===================>] 123.90K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2021-10-01 00:08:07 (1.77 MB/s) - ‘en_ewt-ud-dev.txt’ saved [126870/126870]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jungyeul/computational-tools-for-linguistic-analysis-ubc/main/labs/data/en_ewt-ud-dev-alt.txt\n",
    "!wget https://raw.githubusercontent.com/jungyeul/computational-tools-for-linguistic-analysis-ubc/main/labs/data/en_ewt-ud-dev.txt    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63a469",
   "metadata": {},
   "source": [
    "## 1. tokenization\n",
    "### `tokenizer.sed`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad098fd",
   "metadata": {},
   "source": [
    "First, download the Penn treebank-style tokenization script (`tokenizer.sed`) which is available at ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenizer.sed.\n",
    "\n",
    "Check permissions of `tokenizer.sed` with `ls -l`.\n",
    "`chmod` could be used to set custom permissions to files and directory.\n",
    "`755` could be understood as 3 units which each number represents the permission to read (`r`), write (`w`) and execute (`x`) the file. The first unit represents the permission for the user owner (you); The second unit represents the permission for other owners in the group owning the file; the third unit represents permission for others. \n",
    "Each unit is a sum of the following numbers: \n",
    " - `4`: read\n",
    " - `2`: write\n",
    " - `1`: execute\n",
    " \n",
    "In this case, \n",
    "- \"`7`\" = \"4+2+1\" which means the user owner of the file/directory can read (4), write (2) and execute (1) permissions\n",
    "- \"`5`\" = \"4+0+1\" which means the group owner of the file/directory can read (4), cannot write (0) but can execute (1) permissions\n",
    "- \"`5`\" = same as above but this unit represents permissions for other users\n",
    "Therefore, `chmod 755 tokenizer.sed` signifies changing the permissions of `tokenizer.sed` for its users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a29b1e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-10 14:27:18--  ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenizer.sed\n",
      "           => ‘tokenizer.sed.2’\n",
      "Resolving ftp.cis.upenn.edu (ftp.cis.upenn.edu)... 158.130.67.137\n",
      "Connecting to ftp.cis.upenn.edu (ftp.cis.upenn.edu)|158.130.67.137|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /pub/treebank/public_html ... done.\n",
      "==> SIZE tokenizer.sed ... 2204\n",
      "==> PASV ... done.    ==> RETR tokenizer.sed ... done.\n",
      "Length: 2204 (2.2K) (unauthoritative)\n",
      "\n",
      "tokenizer.sed.2     100%[===================>]   2.15K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-12-10 14:27:19 (64.8 KB/s) - ‘tokenizer.sed.2’ saved [2204]\n",
      "\n",
      "-rwxr-xr-x 1 kkwt kkwt 2290 Sep 21 17:42 tokenizer.sed\n"
     ]
    }
   ],
   "source": [
    "!wget ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenizer.sed\n",
    "!ls -l tokenizer.sed\n",
    "!chmod 755 tokenizer.sed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ef5816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/sed -f\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 tokenizer.sed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78961a71",
   "metadata": {},
   "source": [
    "We are now able to run `tokenizer.sed` to perform tokenization on the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc17613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the AP comes this story : \n",
      "\n",
      "President Bush on Tuesday nominated two individuals to replace retiring jurists \n",
      "on federal courts in the Washington area. Bush nominated Jennifer M. Anderson \n",
      "for a 15-year term as associate judge of the Superior Court of the District of \n",
      "Columbia , replacing Steffen W. Graae. *** Bush also nominated A. Noel Anketell \n",
      "Kramer for a 15-year term as associate judge of the District of Columbia Court \n",
      "of Appeals , replacing John Montague Steadman . \n",
      "\n",
      "The sheikh in wheel-chair has been attacked with a F-16-launched bomb. He could \n",
      "/bin/sed: couldn't write 65 items to stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!./tokenizer.sed en_ewt-ud-dev.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88ab2a",
   "metadata": {},
   "source": [
    "### `tokenizer.perl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586932b9",
   "metadata": {},
   "source": [
    "**TODO** complete commands for `tokenizer.perl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a441104",
   "metadata": {},
   "source": [
    "First, download `scripts.tgz` and extract `tokenizer.perl` from Moses which is available in the at https://www.statmt.org/wmt09/scripts.tgz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f08ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-01 00:08:09--  https://www.statmt.org/wmt09/scripts.tgz\n",
      "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
      "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4927 (4.8K) [application/x-gzip]\n",
      "Saving to: ‘scripts.tgz’\n",
      "\n",
      "scripts.tgz         100%[===================>]   4.81K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-10-01 00:08:10 (33.2 MB/s) - ‘scripts.tgz’ saved [4927/4927]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.statmt.org/wmt09/scripts.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa2b309-22cd-49b1-ac1f-dc0d4f8b9be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/\n",
      "scripts/detokenizer.perl\n",
      "scripts/wrap-xml.perl\n",
      "scripts/lowercase.perl\n",
      "scripts/tokenizer.perl\n",
      "scripts/reuse-weights.perl\n",
      "scripts/nonbreaking_prefixes/\n",
      "scripts/nonbreaking_prefixes/nonbreaking_prefix.de\n",
      "scripts/nonbreaking_prefixes/nonbreaking_prefix.en\n",
      "scripts/nonbreaking_prefixes/nonbreaking_prefix.el\n"
     ]
    }
   ],
   "source": [
    "! tar xvfz scripts.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5093f7e-73aa-471f-8aa7-088f9d0504e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x 1 kkwt kkwt 4207 Sep 27  2007 scripts/tokenizer.perl\n"
     ]
    }
   ],
   "source": [
    "!ls -l scripts/tokenizer.perl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be446bdc",
   "metadata": {},
   "source": [
    "We are now able to run `tokenizer.perl` to perform tokenization on the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b94aaec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer v3\n",
      "Language: en\n",
      "From the AP comes this story :\n",
      "\n",
      "President Bush on Tuesday nominated two individuals to replace retiring jurists\n",
      "on federal courts in the Washington area . Bush nominated Jennifer M. Anderson\n",
      "for a 15-year term as associate judge of the Superior Court of the District of\n",
      "Columbia , replacing Steffen W. Graae . * * * Bush also nominated A. Noel Anketell\n",
      "Kramer for a 15-year term as associate judge of the District of Columbia Court\n",
      "of Appeals , replacing John Montague Steadman .\n",
      "\n",
      "The sheikh in wheel-chair has been attacked with a F-16-launched bomb . He could\n",
      "Unable to flush stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!./scripts/tokenizer.perl -l en < en_ewt-ud-dev.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e4c04c",
   "metadata": {},
   "source": [
    "## 2. sentence boundary detection\n",
    "### `sbd.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a89a60",
   "metadata": {},
   "source": [
    "First, download and extract the sentence splitter `sbd.py` in `splitta-0.1.0.tar.gz` from https://files.pythonhosted.org/packages/cf/d2/9771eb65f1dc3925dbcfc7c4b2adaefa38e1549e4e4e75409df316f8c453/splitta-0.1.0.tar.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c03ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-01 00:08:10--  https://files.pythonhosted.org/packages/cf/d2/9771eb65f1dc3925dbcfc7c4b2adaefa38e1549e4e4e75409df316f8c453/splitta-0.1.0.tar.gz\n",
      "Resolving files.pythonhosted.org (files.pythonhosted.org)... 151.101.193.63, 151.101.1.63, 151.101.65.63, ...\n",
      "Connecting to files.pythonhosted.org (files.pythonhosted.org)|151.101.193.63|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6943603 (6.6M) [application/octet-stream]\n",
      "Saving to: ‘splitta-0.1.0.tar.gz’\n",
      "\n",
      "splitta-0.1.0.tar.g 100%[===================>]   6.62M  6.39MB/s    in 1.0s    \n",
      "\n",
      "2021-10-01 00:08:12 (6.39 MB/s) - ‘splitta-0.1.0.tar.gz’ saved [6943603/6943603]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://files.pythonhosted.org/packages/cf/d2/9771eb65f1dc3925dbcfc7c4b2adaefa38e1549e4e4e75409df316f8c453/splitta-0.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99083c",
   "metadata": {},
   "source": [
    "**TODO** complete commands for `sbd.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a096fc07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitta-0.1.0/\n",
      "splitta-0.1.0/MANIFEST.in\n",
      "splitta-0.1.0/PKG-INFO\n",
      "splitta-0.1.0/README.rst\n",
      "splitta-0.1.0/setup.cfg\n",
      "splitta-0.1.0/setup.py\n",
      "splitta-0.1.0/splitta/\n",
      "splitta-0.1.0/splitta/__init__.py\n",
      "splitta-0.1.0/splitta/model_nb/\n",
      "splitta-0.1.0/splitta/model_nb/feats\n",
      "splitta-0.1.0/splitta/model_nb/lower_words\n",
      "splitta-0.1.0/splitta/model_nb/non_abbrs\n",
      "splitta-0.1.0/splitta/model_svm/\n",
      "splitta-0.1.0/splitta/model_svm/feats\n",
      "splitta-0.1.0/splitta/model_svm/lower_words\n",
      "splitta-0.1.0/splitta/model_svm/non_abbrs\n",
      "splitta-0.1.0/splitta/model_svm/sample.txt\n",
      "splitta-0.1.0/splitta/model_svm/sbd.py\n",
      "splitta-0.1.0/splitta/model_svm/svm_model\n",
      "splitta-0.1.0/splitta/sbd.py\n",
      "splitta-0.1.0/splitta/sbd_util.py\n",
      "splitta-0.1.0/splitta/word_tokenize.py\n",
      "splitta-0.1.0/splitta.egg-info/\n",
      "splitta-0.1.0/splitta.egg-info/dependency_links.txt\n",
      "splitta-0.1.0/splitta.egg-info/PKG-INFO\n",
      "splitta-0.1.0/splitta.egg-info/SOURCES.txt\n",
      "splitta-0.1.0/splitta.egg-info/top_level.txt\n"
     ]
    }
   ],
   "source": [
    "!tar xvfz splitta-0.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70a246a-890b-4e2f-add3-9da8f1a2b22f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kkwt/splitta-0.1.0\n"
     ]
    }
   ],
   "source": [
    "cd splitta-0.1.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668173c1",
   "metadata": {},
   "source": [
    "Since `sbd.py` is a Python script (`.py`), it has to be implemented in a Python terminal to perform SBD. We can access the Python terminal on your system using `python3` or `python2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "324d31be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"/home/kkwt/splitta-0.1.0/splitta-0.1.0/./splitta/sbd.py\", line 537\n",
      "    print '[%d] [%1.4f] %s?? %s' %(frag.label, frag.pred, w1, w2)\n",
      "          ^\n",
      "SyntaxError: invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python3 ./splitta/sbd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f0117f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: sbd.py [options] <text_file>\r\n",
      "\r\n",
      "Options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -v, --verbose         verbose output\r\n",
      "  -t, --tokenize        write tokenized output\r\n",
      "  -m MODEL_PATH, --model=MODEL_PATH\r\n",
      "                        model path\r\n",
      "  -o OUTPUT, --output=OUTPUT\r\n",
      "                        write sentences to this file\r\n",
      "  -x TRAIN, --train=TRAIN\r\n",
      "                        train a new model using this labeled data file\r\n",
      "  -c, --svm             use SVM instead of Naive Bayes for training\r\n"
     ]
    }
   ],
   "source": [
    "!python2 ./splitta/sbd.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a922b",
   "metadata": {},
   "source": [
    "Now, We are able to run `sbd.py`, using `model_svm` with the `-m` option to perform SBD on the tokenized text. \n",
    "Looking into the output, one can see that the text has already been segmented into sentences which each of them ends on a period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f74447",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from [./splitta/model_svm/]... done!\n",
      "reading [../en_ewt-ud-dev.txt]\n",
      "featurizing... done!\n",
      "SVM classifying... done!\n",
      "From the AP comes this story : President Bush on Tuesday nominated two individuals to replace retiring jurists on federal courts in the Washington area.\n",
      "Bush nominated Jennifer M.\n",
      "Anderson for a 15-year term as associate judge of the Superior Court of the District of Columbia, replacing Steffen W.\n",
      "Graae.\n",
      "*** Bush also nominated A.\n",
      "Noel Anketell Kramer for a 15-year term as associate judge of the District of Columbia Court of Appeals, replacing John Montague Steadman.\n",
      "\n",
      "The sheikh in wheel-chair has been attacked with a F-16-launched bomb.\n",
      "He could be killed years ago and the israelians have all the reasons, since he founded and he is the spiritual leader of Hamas, but they didn't.\n",
      "Today's incident proves that Sharon has lost his patience and his hope in peace.\n",
      "Traceback (most recent call last):\n",
      "  File \"./splitta/sbd.py\", line 671, in <module>\n",
      "    test.segment(use_preds=True, tokenize=options.tokenize, output=options.output)\n",
      "  File \"./splitta/sbd.py\", line 517, in segment\n",
      "    elif not list_only: sys.stdout.write(sent_text + spacer)\n",
      "IOError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!python2 ./splitta/sbd.py -m ./splitta/model_svm ../en_ewt-ud-dev.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dc9a8",
   "metadata": {},
   "source": [
    "Yet, it is worth noting that the period in an abbreviation was mistaken as a period that signifies sentence end. For example, the name Jennifer M. Anderson was split into two sentences. \n",
    "\n",
    "Accordingly, one can employ `svm_light` to solve this classification/pattern recognition problem. SVM stands for Support Vector Machine which is a simple supervised machine algorithm that could be used for classification.\n",
    "\n",
    "Download and extract `svm_light` in `svm_light.tar.gz` from http://download.joachims.org/svm_light/current/svm_light.tar.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a8b591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kkwt/splitta-0.1.0/svm_light\n"
     ]
    }
   ],
   "source": [
    "cd svm_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72d385a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-01 00:08:13--  http://download.joachims.org/svm_light/current/svm_light.tar.gz\n",
      "Resolving download.joachims.org (download.joachims.org)... 81.88.34.174, 81.88.42.187\n",
      "Connecting to download.joachims.org (download.joachims.org)|81.88.34.174|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://osmot.cs.cornell.edu/svm_light/current/svm_light.tar.gz [following]\n",
      "--2021-10-01 00:08:14--  http://osmot.cs.cornell.edu/svm_light/current/svm_light.tar.gz\n",
      "Resolving osmot.cs.cornell.edu (osmot.cs.cornell.edu)... 128.253.51.182\n",
      "Connecting to osmot.cs.cornell.edu (osmot.cs.cornell.edu)|128.253.51.182|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 51026 (50K) [application/x-gzip]\n",
      "Saving to: ‘svm_light.tar.gz’\n",
      "\n",
      "svm_light.tar.gz    100%[===================>]  49.83K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-10-01 00:08:14 (356 KB/s) - ‘svm_light.tar.gz’ saved [51026/51026]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.joachims.org/svm_light/current/svm_light.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4627beea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE.txt\n",
      "Makefile\n",
      "svm_learn.c\n",
      "kernel.h\n",
      "svm_learn.h\n",
      "svm_learn_main.c\n",
      "svm_classify.c\n",
      "svm_loqo.c\n",
      "svm_common.c\n",
      "svm_common.h\n",
      "svm_hideo.c\n"
     ]
    }
   ],
   "source": [
    "!tar xvfz svm_light.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db44a89",
   "metadata": {},
   "source": [
    "In the svm_light directory, run `make all` to compile the svm-light programme along with all of the required dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f97e9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -c  -O3                      svm_learn_main.c -o svm_learn_main.o \n",
      "gcc -c  -O3                      svm_learn.c -o svm_learn.o \n",
      "gcc -c  -O3                      svm_common.c -o svm_common.o \n",
      "\u001b[01m\u001b[Ksvm_common.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kread_model\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksvm_common.c:600:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  600 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"SVM-light Version %s\\n\",version_buffer)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:605:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  605 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%ld%*[^\\n]\\n\", &model->kernel_parm.kernel_type)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:606:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  606 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%ld%*[^\\n]\\n\", &model->kernel_parm.poly_degree)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:607:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  607 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%lf%*[^\\n]\\n\", &model->kernel_parm.rbf_gamma)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:608:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  608 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%lf%*[^\\n]\\n\", &model->kernel_parm.coef_lin)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:609:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  609 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%lf%*[^\\n]\\n\", &model->kernel_parm.coef_const)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:610:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  610 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%[^#]%*[^\\n]\\n\", model->kernel_parm.custom)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:612:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  612 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%ld%*[^\\n]\\n\", &model->totwords)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:613:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  613 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%ld%*[^\\n]\\n\", &model->totdoc)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:614:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  614 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%ld%*[^\\n]\\n\", &model->sv_num)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:615:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfscanf\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  615 |   \u001b[01;35m\u001b[Kfscanf(modelfl,\"%lf%*[^\\n]\\n\", &model->b)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksvm_common.c:623:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfgets\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "  623 |     \u001b[01;35m\u001b[Kfgets(line,(int)ll,modelfl)\u001b[m\u001b[K;\n",
      "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "gcc -c  -O3                      svm_hideo.c -o svm_hideo.o \n",
      "gcc  -O3                      svm_learn_main.o svm_learn.o svm_common.o svm_hideo.o -o svm_learn -L. -lm                               \n",
      "gcc -c  -O3                      svm_classify.c -o svm_classify.o\n",
      "gcc  -O3                      svm_classify.o svm_common.o -o svm_classify -L. -lm                               \n"
     ]
    }
   ],
   "source": [
    "!make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e217ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_classify\n"
     ]
    }
   ],
   "source": [
    "!ls svm_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db5bc4b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kkwt/splitta-0.1.0/splitta-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924ea07",
   "metadata": {},
   "source": [
    "Now, we are able to perform SBD along with SVM-light on the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a97e472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kkwt/splitta-0.1.0\n"
     ]
    }
   ],
   "source": [
    "cd ~/splitta-0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f1d524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from [./splitta/model_svm/]... done!\n",
      "reading [../en_ewt-ud-dev.txt]\n",
      "featurizing... done!\n",
      "SVM classifying... done!\n"
     ]
    }
   ],
   "source": [
    "!python2 ./splitta/sbd.py -m ./splitta/model_svm ../en_ewt-ud-dev.txt -o ../en_ewt-ud-dev.txt.sbd_splitta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1132c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from [./splitta/model_svm/]... done!\n",
      "reading [../en_ewt-ud-dev.txt]\n",
      "featurizing... done!\n",
      "SVM classifying... done!\n"
     ]
    }
   ],
   "source": [
    "!python2 ./splitta/sbd.py -m ./splitta/model_svm ../en_ewt-ud-dev.txt > ../en_ewt-ud-dev.txt.sbd_splitta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
