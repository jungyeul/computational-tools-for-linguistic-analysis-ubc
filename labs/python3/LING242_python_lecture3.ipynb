{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LING 242 Python Lecture 3: Strings\n",
    "\n",
    "* Word \"Lists\" (Sets)\n",
    "* Simple dictionary lexicons\n",
    "* Complex lexicons\n",
    "* Lexicons in NLTK\n",
    "* Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word \"Lists\" (Sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The simpliest lexicon is just a list of words that have something in common. For example:\n",
    "\n",
    "* Pronouns (\"he\",\"she\", \"I\", ...)\n",
    "* Negative words (\"terrible\",\"jerk\",\"foolishly\",...)\n",
    "* A list of all family relations (\"father\", \"sister\",...)\n",
    "* The vocabulary of the Brown corpus\n",
    "\n",
    "Typically, one builds a lexicon so that one can identify instances of this word class in some corpus\n",
    "\n",
    "Rule #1: Don't use lists for lexicons. Use [sets](https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words = [\"the\",\"quick\",\"brown\", \"fox\",\"jumped\", \"over\",\"the\",\"lazy\",\"dog\"]\n",
    "\n",
    "word_types = set(some_words)\n",
    "word_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Two reasons to use sets for lexicons:\n",
    "\n",
    "* Elements are unique, which is exactly what you need for lexicons\n",
    "* Checking for membership (*in*) is much faster, $O(1)$ vs $O(n)$. This especially matters when the lexicon is big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a few ways to create sets. \n",
    "- If you are starting totally from scratch, you have to use the build-in *set()* function\n",
    "    - curly brackets alone refer to dicts, not sets! \n",
    "- If you are starting with a (small) fixed set of existing items you can declare using curly brackets instead of converting a list. \n",
    "- There are also set comprehensions, also using curly brackets!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "seasons = {\"spring\",\"summer\",\"fall\"}\n",
    "months = {}\n",
    "days = {n for n in range(31)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall', 'spring', 'summer', 'winter'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasons.add(\"winter\")\n",
    "seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}\n"
     ]
    }
   ],
   "source": [
    "days.add(31)\n",
    "print(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n9/3r6hrp015t9d1n6m8l36t0yh0000gn/T/ipykernel_28846/2444808002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmonths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"January\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "months.add(\"January\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'January'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months = set()\n",
    "months.add(\"January\")\n",
    "months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is worth memorizing the basic set methods for adding and removing items, including `add`, `update`, and `discard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "planets = {\"Mars\",\"Venus\", \"Mercury\"}\n",
    "more_planets = [\"Jupiter\",\"Neptune,\",\"Uranus\", \"Pluto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Earth', 'Mars', 'Mercury', 'Venus'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planets.add(\"Earth\")\n",
    "planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Earth', 'Jupiter', 'Mars', 'Mercury', 'Neptune,', 'Pluto', 'Uranus', 'Venus'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planets.update(more_planets)\n",
    "planets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Earth', 'Jupiter', 'Mars', 'Mercury', 'Neptune,', 'Uranus', 'Venus'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planets.discard(\"Pluto\")\n",
    "planets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sets are great for quickly finding intersections using `&` and differences using `-`. These are WAY faster than alternatives involving looping. Let's find words only in both the Brown and Treebank, and words that appear in only one or the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown vocab:  56057\n",
      "treebank vocab:  12408\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown, treebank\n",
    "\n",
    "brown_vocab = set(brown.words())\n",
    "print('brown vocab: ', len(brown_vocab))\n",
    "treebank_vocab = set(treebank.words())\n",
    "print('treebank vocab: ', len(treebank_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47510"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown_vocab - treebank_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3861"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treebank_vocab - brown_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8547"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown_vocab & treebank_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You might sometimes want to intersect a set with something that isn't a set (like a list, or even a string). If so, and it doesn't make sense to convert the other to a set, you can use set methods such as `intersection` instead of operators (which only work when both elements are sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = {'a','e','i','o','u','y'}\n",
    "word1 = \"rstln\"\n",
    "word2 = \"rstlne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels.intersection(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vowels.intersection(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'set' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n9/3r6hrp015t9d1n6m8l36t0yh0000gn/T/ipykernel_28846/343403993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvowels\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mword1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'set' and 'str'"
     ]
    }
   ],
   "source": [
    "vowels & word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1_set = set(word1)\n",
    "vowels & word1_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some set drawbacks:\n",
    "\n",
    "* No order, no guarantee that order will be perserved when the set changes\n",
    "* Can't add mutuable objects like lists and dicts (use tuples!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mar', 'Feb', 'Jan']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months = [\"Feb\", \"Jan\", \"Mar\"]\n",
    "list(set(months))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple dictionary lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most common lexical need in computational linguistics: word counts\n",
    "\n",
    "Easy way to build a dictionary of counts when you've got a list (or other iterable) of words: [Counters](https://docs.python.org/3/library/collections.html#collections.Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62713"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can `update` counters to add more items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66758"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.update(treebank.words())\n",
    "counts[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But sometimes not practical because what you are counting isn't an iterable or you want to do some operation before counting. A normal Python dict is fine, but each value needs to be initialized to zero. One solution: the [get](https://docs.python.org/3/library/stdtypes.html#dict.get) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69971"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {}\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    #if word not in counts:\n",
    "    #    counts[word] = 0\n",
    "    counts[word] = counts.get(word, 0) + 1\n",
    "    \n",
    "counts[\"the\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A quick exercise: let's count the first words of sentences in the Brown corpus using `get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789\n"
     ]
    }
   ],
   "source": [
    "first_word_count = {}\n",
    "for sent in brown.sents():\n",
    "    first_word_count[sent[0]] = first_word_count.get(sent[0], 0) + 1\n",
    "    \n",
    "print(first_word_count[\"And\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another solution to the initialization problem for counting (and other situations) is the [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict). When you initialize a defaultdict for integers, the default value is zero, no need to do anything but add! Defaultdicts also work for lists, sets, and dicts (creating an empty instance as soon as you try to access it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62713\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "\n",
    "counts = defaultdict(int)\n",
    "\n",
    "for word in brown.words():\n",
    "    counts[word] += 1\n",
    "    \n",
    "print(counts[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another very common need: assign a index (an integer) to each word, for looking up in data structures like matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10449"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict = {}\n",
    "for word in counts:\n",
    "    index_dict[word] = len(index_dict)\n",
    "    \n",
    "index_dict[\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In these cases, you will often need a reversed index as well. The `items` method for dictionaries is useful if you are iterating over an existing dictionary and want to access both key and value at the same time. Let's build a reverse index dict in one line using a dictionary comprehension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_index_dict = {value:key for key, value in index_dict.items()}\n",
    "\n",
    "rev_index_dict[10449]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Complex lexicons\n",
    "\n",
    "\n",
    "Sometimes lexicons are more complex and might be represented as multiple recursive Python datatypes. For example, instead of a single counts, you might have a list of word senses, which are actually dictionaries of properties (including part-of-speech and counts of that word sense in a corpus). More about word senses in COLX 561!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mini_sense_lexicon = {\"bear\":[{\"POS\":\"noun\",\"animate\":True,\"count\":634,\"gloss\":\"A big furry animal\"},\n",
    "                              {\"POS\":\"verb\",\"transitive\":True,\"count\":294, \"past tense\":\"bore\", \"past participle\":\"borne\", \"gloss\":\"to endure\"}],\n",
    "                      \"slug\":[{\"POS\":\"noun\",\"animate\":True,\"count\":34, \"gloss\":\"A slimy animal\"},\n",
    "                              {\"POS\":\"verb\",\"transitive\":True,\"count\":3, \"gloss\": \"to hit\"}],\n",
    "                      \"back\":[{\"POS\":\"noun\",\"animate\":False,\"count\":12,\"gloss\":\"a body part\"},\n",
    "                              {\"POS\":\"noun\",\"animate\":False,\"count\":43, \"gloss\":\"the rear of a place\"},\n",
    "                              {\"POS\":\"verb\",\"transitive\":True,\"count\":5, \"gloss\":\"to support\"},\n",
    "                              {\"POS\":\"adverb\",\"count\":47,\"gloss\":\"in a returning fashion\"}],\n",
    "                      \"good\":[{\"POS\":\"noun\",\"animate\":False,\"count\":19,\"gloss\":\"a thing of value\"},\n",
    "                              {\"POS\":\"adjective\", \"count\":1293,\"gloss\":\"positive\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These can be tricky to navigate. Let's answer the following questions by accessing the information in data structure:\n",
    "\n",
    "How many senses does \"back\" have in this lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_sense_lexicon[\"back\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Does \"slug\" have an adjectival sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(feature_dict[\"POS\"] == \"adjective\" for feature_dict in mini_sense_lexicon[\"slug\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which words have a verb sense with an irregular past tense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear\n"
     ]
    }
   ],
   "source": [
    "for word, pos_list in mini_sense_lexicon.items():\n",
    "    for feature_dict in pos_list:\n",
    "        if \"past tense\" in feature_dict:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the gloss of the most common sense of \"back\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in a returning fashion'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_count = 0\n",
    "gloss = \"\"\n",
    "\n",
    "for feature_dict in mini_sense_lexicon[\"back\"]:\n",
    "    if feature_dict[\"count\"] > highest_count:\n",
    "        highest_count = feature_dict[\"count\"]\n",
    "        gloss = feature_dict[\"gloss\"]\n",
    "gloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLTK lexicons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLTK has a lot of useful lexicons. Most are word lists. Note that they are listed under corpora and have the same interface; note that they need to be converted to sets if you want to use them for look up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Stopwords\n",
    "\n",
    "Lists of closed-class/function words in various languages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"french\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Swadesh\n",
    "\n",
    "words for 200 common concepts from large list of languages, used for historical linguistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package swadesh to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package swadesh is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('swadesh')\n",
    "\n",
    "from nltk.corpus import swadesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'bg',\n",
       " 'bs',\n",
       " 'ca',\n",
       " 'cs',\n",
       " 'cu',\n",
       " 'de',\n",
       " 'en',\n",
       " 'es',\n",
       " 'fr',\n",
       " 'hr',\n",
       " 'it',\n",
       " 'la',\n",
       " 'mk',\n",
       " 'nl',\n",
       " 'pl',\n",
       " 'pt',\n",
       " 'ro',\n",
       " 'ru',\n",
       " 'sk',\n",
       " 'sl',\n",
       " 'sr',\n",
       " 'sw',\n",
       " 'uk']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swadesh.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ich\n",
      "you (singular), thou du, Sie\n",
      "he er\n",
      "we wir\n",
      "you (plural) ihr, Sie\n",
      "they sie\n",
      "this dieses\n",
      "that jenes\n",
      "here hier\n",
      "there dort\n",
      "who wer\n",
      "what was\n",
      "where wo\n",
      "when wann\n",
      "how wie\n",
      "not nicht\n",
      "all alle\n",
      "many viele\n",
      "some einige\n",
      "few wenige\n",
      "other andere\n",
      "one eins\n",
      "two zwei\n",
      "three drei\n",
      "four vier\n",
      "five fünf\n",
      "big groß\n",
      "long lang\n",
      "wide breit, weit\n",
      "thick dick\n",
      "heavy schwer\n",
      "small klein\n",
      "short kurz\n",
      "narrow eng\n",
      "thin dünn\n",
      "woman Frau\n",
      "man (adult male) Mann\n",
      "man (human being) Mensch\n",
      "child Kind\n",
      "wife Frau, Ehefrau\n",
      "husband Mann, Ehemann\n",
      "mother Mutter\n",
      "father Vater\n",
      "animal Tier\n",
      "fish Fisch\n",
      "bird Vogel\n",
      "dog Hund\n",
      "louse Laus\n",
      "snake Schlange\n",
      "worm Wurm\n",
      "tree Baum\n",
      "forest Wald\n",
      "stick Stock\n",
      "fruit Frucht\n",
      "seed Samen\n",
      "leaf Blatt\n",
      "root Wurzel\n",
      "bark (from tree) Rinde\n",
      "flower Blume\n",
      "grass Gras\n",
      "rope Seil\n",
      "skin Haut\n",
      "meat Fleisch\n",
      "blood Blut\n",
      "bone Knochen\n",
      "fat (noun) Fett\n",
      "egg Ei\n",
      "horn Horn\n",
      "tail Schwanz\n",
      "feather Feder\n",
      "hair Haar\n",
      "head Kopf, Haupt\n",
      "ear Ohr\n",
      "eye Auge\n",
      "nose Nase\n",
      "mouth Mund\n",
      "tooth Zahn\n",
      "tongue Zunge\n",
      "fingernail Fingernagel\n",
      "foot Fuß\n",
      "leg Bein\n",
      "knee Knie\n",
      "hand Hand\n",
      "wing Flügel\n",
      "belly Bauch\n",
      "guts Eingeweide, Innereien\n",
      "neck Hals\n",
      "back Rücken\n",
      "breast Brust\n",
      "heart Herz\n",
      "liver Leber\n",
      "drink trinken\n",
      "eat essen\n",
      "bite beißen\n",
      "suck saugen\n",
      "spit spucken\n",
      "vomit erbrechen\n",
      "blow blasen\n",
      "breathe atmen\n",
      "laugh lachen\n",
      "see sehen\n",
      "hear hören\n",
      "know (a fact) wissen\n",
      "think denken\n",
      "smell riechen\n",
      "fear fürchten\n",
      "sleep schlafen\n",
      "live leben\n",
      "die sterben\n",
      "kill töten\n",
      "fight kämpfen\n",
      "hunt jagen\n",
      "hit schlagen\n",
      "cut schneiden\n",
      "split spalten\n",
      "stab stechen\n",
      "scratch kratzen\n",
      "dig graben\n",
      "swim schwimmen\n",
      "fly (verb) fliegen\n",
      "walk gehen\n",
      "come kommen\n",
      "lie liegen\n",
      "sit sitzen\n",
      "stand stehen\n",
      "turn drehen\n",
      "fall fallen\n",
      "give geben\n",
      "hold halten\n",
      "squeeze quetschen\n",
      "rub reiben\n",
      "wash waschen\n",
      "wipe wischen\n",
      "pull ziehen\n",
      "push drücken\n",
      "throw werfen\n",
      "tie binden\n",
      "sew nähen\n",
      "count zählen\n",
      "say sagen\n",
      "sing singen\n",
      "play spielen\n",
      "float schweben\n",
      "flow fließen\n",
      "freeze frieren\n",
      "swell schwellen\n",
      "sun Sonne\n",
      "moon Mond\n",
      "star Stern\n",
      "water Wasser\n",
      "rain Regen\n",
      "river Fluß\n",
      "lake See\n",
      "sea Meer, See\n",
      "salt Salz\n",
      "stone Stein\n",
      "sand Sand\n",
      "dust Staub\n",
      "earth Erde\n",
      "cloud Wolke\n",
      "fog Nebel\n",
      "sky Himmel\n",
      "wind Wind\n",
      "snow Schnee\n",
      "ice Eis\n",
      "smoke Rauch\n",
      "fire Feuer\n",
      "ashes Asche\n",
      "burn brennen\n",
      "road Straße\n",
      "mountain Berg\n",
      "red rot\n",
      "green grün\n",
      "yellow gelb\n",
      "white weiß\n",
      "black schwarz\n",
      "night Nacht\n",
      "day Tag\n",
      "year Jahr\n",
      "warm warm\n",
      "cold kalt\n",
      "full voll\n",
      "new neu\n",
      "old alt\n",
      "good gut\n",
      "bad schlecht\n",
      "rotten verrottet\n",
      "dirty schmutzig\n",
      "straight gerade\n",
      "round rund\n",
      "sharp scharf\n",
      "dull stumpf\n",
      "smooth glatt\n",
      "wet nass, feucht\n",
      "dry trocken\n",
      "correct richtig\n",
      "near nah, nahe\n",
      "far weit, fern\n",
      "right rechts\n",
      "left links\n",
      "at bei, an\n",
      "in in\n",
      "with mit\n",
      "and und\n",
      "if wenn, falls, ob\n",
      "because weil\n",
      "name Name\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(swadesh.words(\"en\"))):\n",
    "    print(swadesh.words(\"en\")[i], swadesh.words(\"de\")[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Names\n",
    "\n",
    "Lists of mostly English names, divided by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('names')\n",
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zonnya',\n",
       " 'Zora',\n",
       " 'Zorah',\n",
       " 'Zorana',\n",
       " 'Zorina',\n",
       " 'Zorine',\n",
       " 'Zsa Zsa',\n",
       " 'Zsazsa',\n",
       " 'Zulema',\n",
       " 'Zuzana']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.words(\"female.txt\")[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Opinion Lexicon\n",
    "\n",
    "Positive and negative word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative-words.txt', 'positive-words.txt']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.words('negative-words.txt')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CMU Pronouncing Dictionary\n",
    "\n",
    "* A list of pronunciations for each English word string\n",
    "* Pronunciations are a list of ARPAbet phones\n",
    "* ARPAbet phones are strings which are alphabetic except for numbers at the end of the vowels\n",
    "* The numbers indicate stress\n",
    "\n",
    "Let's look at a few entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"cmudict\")\n",
    "from nltk.corpus import cmudict\n",
    "p_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['R', 'EH1', 'D'], ['R', 'IY1', 'D']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_dict[\"read\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['IH1', 'N', 'D', 'EH0', 'K', 'S']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_dict[\"index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's get some basic stats for this lexicon: total entries, average number of pronunciations per word, average number of phones per pronunciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pro_count = 0\n",
    "pho_count = 0\n",
    "for pronuns in p_dict.values():\n",
    "    pro_count += len(pronuns)\n",
    "    for pronun in pronuns:\n",
    "        pho_count += len(pronun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123455"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0832854076384109"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_count/len(p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3850542482633825"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pho_count/pro_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a set of female names that appear in the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abbe',\n",
       " 'Abbey',\n",
       " 'Abigail',\n",
       " 'Abra',\n",
       " 'Ada',\n",
       " 'Adele',\n",
       " 'Adrian',\n",
       " 'Adrien',\n",
       " 'Agatha',\n",
       " 'Aggie',\n",
       " 'Agnes',\n",
       " 'Agnese',\n",
       " 'Aida',\n",
       " 'Ailey',\n",
       " 'Ainsley',\n",
       " 'Alex',\n",
       " 'Alexis',\n",
       " 'Alice',\n",
       " 'Alicia',\n",
       " 'Alison',\n",
       " 'Alix',\n",
       " 'Alla',\n",
       " 'Allison',\n",
       " 'Alma',\n",
       " 'Althea',\n",
       " 'Amy',\n",
       " 'Ana',\n",
       " 'Anabel',\n",
       " 'Andrea',\n",
       " 'Andrei',\n",
       " 'Andromache',\n",
       " 'Andy',\n",
       " 'Angel',\n",
       " 'Angelina',\n",
       " 'Angie',\n",
       " 'Anita',\n",
       " 'Ann',\n",
       " 'Anna',\n",
       " 'Anne',\n",
       " 'Annie',\n",
       " 'Ansley',\n",
       " 'Anthea',\n",
       " 'Antoinette',\n",
       " 'Aphrodite',\n",
       " 'April',\n",
       " 'Arden',\n",
       " 'Ariadne',\n",
       " 'Arlen',\n",
       " 'Arlene',\n",
       " 'Ashley',\n",
       " 'Asia',\n",
       " 'Astra',\n",
       " 'Athena',\n",
       " 'Atlanta',\n",
       " 'Audrey',\n",
       " 'Augusta',\n",
       " 'Augustine',\n",
       " 'Aurora',\n",
       " 'Austin',\n",
       " 'Avis',\n",
       " 'Bambi',\n",
       " 'Barbara',\n",
       " 'Bari',\n",
       " 'Barry',\n",
       " 'Bea',\n",
       " 'Beatrice',\n",
       " 'Beau',\n",
       " 'Bee',\n",
       " 'Bel',\n",
       " 'Bell',\n",
       " 'Bella',\n",
       " 'Belle',\n",
       " 'Benita',\n",
       " 'Benny',\n",
       " 'Bernardine',\n",
       " 'Bernie',\n",
       " 'Berry',\n",
       " 'Bert',\n",
       " 'Bertha',\n",
       " 'Beryl',\n",
       " 'Bess',\n",
       " 'Bessie',\n",
       " 'Beth',\n",
       " 'Betsey',\n",
       " 'Betsy',\n",
       " 'Betty',\n",
       " 'Beverly',\n",
       " 'Bill',\n",
       " 'Billie',\n",
       " 'Billy',\n",
       " 'Bird',\n",
       " 'Birdie',\n",
       " 'Birgit',\n",
       " 'Birgitta',\n",
       " 'Blair',\n",
       " 'Blake',\n",
       " 'Blanche',\n",
       " 'Bo',\n",
       " 'Bobbie',\n",
       " 'Bobby',\n",
       " 'Bonnie',\n",
       " 'Brett',\n",
       " 'Bride',\n",
       " 'Bridget',\n",
       " 'Brittany',\n",
       " 'Brook',\n",
       " 'Brooke',\n",
       " 'Brooks',\n",
       " 'Bryn',\n",
       " 'Bunny',\n",
       " 'Cal',\n",
       " 'Cam',\n",
       " 'Cameo',\n",
       " 'Cami',\n",
       " 'Camilla',\n",
       " 'Camille',\n",
       " 'Canada',\n",
       " 'Candide',\n",
       " 'Candy',\n",
       " 'Carey',\n",
       " 'Carla',\n",
       " 'Carlisle',\n",
       " 'Carmen',\n",
       " 'Carmine',\n",
       " 'Carol',\n",
       " 'Carolina',\n",
       " 'Caroline',\n",
       " 'Carolyn',\n",
       " 'Caron',\n",
       " 'Carrie',\n",
       " 'Carroll',\n",
       " 'Casey',\n",
       " 'Cat',\n",
       " 'Catherine',\n",
       " 'Cathy',\n",
       " 'Cecil',\n",
       " 'Cecilia',\n",
       " 'Ceil',\n",
       " 'Celia',\n",
       " 'Celie',\n",
       " 'Charity',\n",
       " 'Charlotte',\n",
       " 'Cherry',\n",
       " 'Chris',\n",
       " 'Christi',\n",
       " 'Christian',\n",
       " 'Christiana',\n",
       " 'Christie',\n",
       " 'Christine',\n",
       " 'Christy',\n",
       " 'Chrysler',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'Clara',\n",
       " 'Clare',\n",
       " 'Claude',\n",
       " 'Clemence',\n",
       " 'Cleva',\n",
       " 'Cody',\n",
       " 'Coletta',\n",
       " 'Conchita',\n",
       " 'Connie',\n",
       " 'Constance',\n",
       " 'Constantine',\n",
       " 'Coral',\n",
       " 'Courtenay',\n",
       " 'Courtney',\n",
       " 'Cris',\n",
       " 'Crystal',\n",
       " 'Cynthia',\n",
       " 'Dale',\n",
       " 'Dallas',\n",
       " 'Dana',\n",
       " 'Daniel',\n",
       " 'Danny',\n",
       " 'Daphne',\n",
       " 'Darlene',\n",
       " 'Dawn',\n",
       " 'Deane',\n",
       " 'Debora',\n",
       " 'Del',\n",
       " 'Delaney',\n",
       " 'Delia',\n",
       " 'Della',\n",
       " 'Deloris',\n",
       " 'Delphine',\n",
       " 'Denny',\n",
       " 'Di',\n",
       " 'Diana',\n",
       " 'Diane',\n",
       " 'Didi',\n",
       " 'Dion',\n",
       " 'Dixie',\n",
       " 'Doe',\n",
       " 'Dolley',\n",
       " 'Dolly',\n",
       " 'Dolores',\n",
       " 'Dominique',\n",
       " 'Donna',\n",
       " 'Dora',\n",
       " 'Dorcas',\n",
       " 'Doria',\n",
       " 'Doris',\n",
       " 'Dorothy',\n",
       " 'Dove',\n",
       " 'Drew',\n",
       " 'Easter',\n",
       " 'Eddie',\n",
       " 'Eddy',\n",
       " 'Eden',\n",
       " 'Edith',\n",
       " 'Edna',\n",
       " 'Edwina',\n",
       " 'Edythe',\n",
       " 'Effie',\n",
       " 'Eileen',\n",
       " 'Elaine',\n",
       " 'Eleanor',\n",
       " 'Electra',\n",
       " 'Elena',\n",
       " 'Elinor',\n",
       " 'Elisabeth',\n",
       " 'Elisha',\n",
       " 'Elizabeth',\n",
       " 'Ella',\n",
       " 'Ellen',\n",
       " 'Ellie',\n",
       " 'Elmira',\n",
       " 'Eloise',\n",
       " 'Elsie',\n",
       " 'Elsinore',\n",
       " 'Emma',\n",
       " 'Estella',\n",
       " 'Esther',\n",
       " 'Ethel',\n",
       " 'Eugenia',\n",
       " 'Eve',\n",
       " 'Evelyn',\n",
       " 'Faith',\n",
       " 'Fan',\n",
       " 'Fanny',\n",
       " 'Fatima',\n",
       " 'Fay',\n",
       " 'Fayette',\n",
       " 'Felice',\n",
       " 'Felicity',\n",
       " 'Fidelity',\n",
       " 'Flor',\n",
       " 'Florence',\n",
       " 'Florida',\n",
       " 'Flory',\n",
       " 'Flower',\n",
       " 'Fortune',\n",
       " 'Fran',\n",
       " 'France',\n",
       " 'Frances',\n",
       " 'Francesca',\n",
       " 'Francis',\n",
       " 'Frank',\n",
       " 'Frankie',\n",
       " 'Franny',\n",
       " 'Fred',\n",
       " 'Freddie',\n",
       " 'Freddy',\n",
       " 'Freida',\n",
       " 'Gabriel',\n",
       " 'Gabrielle',\n",
       " 'Galina',\n",
       " 'Garland',\n",
       " 'Gates',\n",
       " 'Gay',\n",
       " 'Gaynor',\n",
       " 'Gene',\n",
       " 'Geneva',\n",
       " 'Genevieve',\n",
       " 'George',\n",
       " 'Georgia',\n",
       " 'Geraldine',\n",
       " 'Gerry',\n",
       " 'Gertrude',\n",
       " 'Gill',\n",
       " 'Gisele',\n",
       " 'Giselle',\n",
       " 'Glad',\n",
       " 'Glen',\n",
       " 'Glenda',\n",
       " 'Glenn',\n",
       " 'Gloria',\n",
       " 'Gloriana',\n",
       " 'Glory',\n",
       " 'Golda',\n",
       " 'Grace',\n",
       " 'Gracie',\n",
       " 'Gray',\n",
       " 'Greer',\n",
       " 'Gretchen',\n",
       " 'Gus',\n",
       " 'Gwen',\n",
       " 'Hannah',\n",
       " 'Hannibal',\n",
       " 'Happy',\n",
       " 'Harmony',\n",
       " 'Harriet',\n",
       " 'Hattie',\n",
       " 'Hazel',\n",
       " 'Heather',\n",
       " 'Hedda',\n",
       " 'Helen',\n",
       " 'Helena',\n",
       " 'Helene',\n",
       " 'Henrietta',\n",
       " 'Hephzibah',\n",
       " 'Hester',\n",
       " 'Hettie',\n",
       " 'Hetty',\n",
       " 'Hildy',\n",
       " 'Hillary',\n",
       " 'Holley',\n",
       " 'Honey',\n",
       " 'Honor',\n",
       " 'Hope',\n",
       " 'Ida',\n",
       " 'Ike',\n",
       " 'Ilka',\n",
       " 'Ilona',\n",
       " 'Inna',\n",
       " 'Ira',\n",
       " 'Irene',\n",
       " 'Irina',\n",
       " 'Irma',\n",
       " 'Isabel',\n",
       " 'Isis',\n",
       " 'Ivory',\n",
       " 'Ivy',\n",
       " 'Jackie',\n",
       " 'Jacky',\n",
       " 'Jacqueline',\n",
       " 'Jan',\n",
       " 'Jana',\n",
       " 'Jane',\n",
       " 'Janet',\n",
       " 'Janice',\n",
       " 'Janis',\n",
       " 'Jean',\n",
       " 'Jeannie',\n",
       " 'Jen',\n",
       " 'Jena',\n",
       " 'Jenni',\n",
       " 'Jennie',\n",
       " 'Jennifer',\n",
       " 'Jenny',\n",
       " 'Jerry',\n",
       " 'Jess',\n",
       " 'Jesse',\n",
       " 'Jessica',\n",
       " 'Jessie',\n",
       " 'Jessy',\n",
       " 'Jinny',\n",
       " 'Jo',\n",
       " 'Joan',\n",
       " 'Joanne',\n",
       " 'Jody',\n",
       " 'Joey',\n",
       " 'Jordan',\n",
       " 'Joyce',\n",
       " 'Juanita',\n",
       " 'Jude',\n",
       " 'Judith',\n",
       " 'Judy',\n",
       " 'Julia',\n",
       " 'Julie',\n",
       " 'Juliet',\n",
       " 'June',\n",
       " 'Justine',\n",
       " 'Kali',\n",
       " 'Kare',\n",
       " 'Karen',\n",
       " 'Karol',\n",
       " 'Kate',\n",
       " 'Katharine',\n",
       " 'Katherine',\n",
       " 'Kathleen',\n",
       " 'Kathy',\n",
       " 'Katie',\n",
       " 'Katya',\n",
       " 'Kay',\n",
       " 'Kelly',\n",
       " 'Kelsey',\n",
       " 'Kerry',\n",
       " 'Kimberly',\n",
       " 'Kira',\n",
       " 'Kirby',\n",
       " 'Kitti',\n",
       " 'Kitty',\n",
       " 'Kizzie',\n",
       " 'La',\n",
       " 'Lacy',\n",
       " 'Lamb',\n",
       " 'Lana',\n",
       " 'Lane',\n",
       " 'Lark',\n",
       " 'Laura',\n",
       " 'Laurel',\n",
       " 'Lauren',\n",
       " 'Lauri',\n",
       " 'Laurie',\n",
       " 'Lee',\n",
       " 'Leigh',\n",
       " 'Leila',\n",
       " 'Leland',\n",
       " 'Lena',\n",
       " 'Leona',\n",
       " 'Leone',\n",
       " 'Leonore',\n",
       " 'Leslie',\n",
       " 'Letitia',\n",
       " 'Lil',\n",
       " 'Lila',\n",
       " 'Lilian',\n",
       " 'Lillian',\n",
       " 'Lilly',\n",
       " 'Lily',\n",
       " 'Linda',\n",
       " 'Lindsay',\n",
       " 'Lindy',\n",
       " 'Lisa',\n",
       " 'Lissa',\n",
       " 'Liz',\n",
       " 'Lizzie',\n",
       " 'Lizzy',\n",
       " 'Lois',\n",
       " 'Lola',\n",
       " 'Lolly',\n",
       " 'Lorain',\n",
       " 'Lorelei',\n",
       " 'Loren',\n",
       " 'Lorena',\n",
       " 'Lorraine',\n",
       " 'Lotte',\n",
       " 'Lottie',\n",
       " 'Lou',\n",
       " 'Louisa',\n",
       " 'Louise',\n",
       " 'Love',\n",
       " 'Lucia',\n",
       " 'Lucille',\n",
       " 'Lucky',\n",
       " 'Lucretia',\n",
       " 'Lucy',\n",
       " 'Luisa',\n",
       " 'Luise',\n",
       " 'Lura',\n",
       " 'Lust',\n",
       " 'Lydia',\n",
       " 'Lynn',\n",
       " 'Maddalena',\n",
       " 'Madeleine',\n",
       " 'Madonna',\n",
       " 'Mae',\n",
       " 'Mag',\n",
       " 'Magdalene',\n",
       " 'Maggie',\n",
       " 'Malia',\n",
       " 'Mallory',\n",
       " 'Mame',\n",
       " 'Manon',\n",
       " 'Marcile',\n",
       " 'Margaret',\n",
       " 'Margo',\n",
       " 'Maria',\n",
       " 'Marie',\n",
       " 'Marietta',\n",
       " 'Marilyn',\n",
       " 'Marin',\n",
       " 'Marina',\n",
       " 'Marion',\n",
       " 'Maris',\n",
       " 'Marjorie',\n",
       " 'Marlene',\n",
       " 'Marsha',\n",
       " 'Martha',\n",
       " 'Marty',\n",
       " 'Mary',\n",
       " 'Matilda',\n",
       " 'Mattie',\n",
       " 'Maude',\n",
       " 'Maureen',\n",
       " 'Maurine',\n",
       " 'Mavis',\n",
       " 'Max',\n",
       " 'Maxine',\n",
       " 'May',\n",
       " 'Mead',\n",
       " 'Meg',\n",
       " 'Mel',\n",
       " 'Melisande',\n",
       " 'Melissa',\n",
       " 'Melody',\n",
       " 'Mercedes',\n",
       " 'Mercy',\n",
       " 'Meredith',\n",
       " 'Merle',\n",
       " 'Merrill',\n",
       " 'Merry',\n",
       " 'Mickie',\n",
       " 'Midge',\n",
       " 'Mignon',\n",
       " 'Millie',\n",
       " 'Mimi',\n",
       " 'Minerva',\n",
       " 'Minnie',\n",
       " 'Mira',\n",
       " 'Miranda',\n",
       " 'Miriam',\n",
       " 'Missy',\n",
       " 'Moll',\n",
       " 'Mollie',\n",
       " 'Molly',\n",
       " 'Mommy',\n",
       " 'Monica',\n",
       " 'Morgan',\n",
       " 'Myra',\n",
       " 'Nadine',\n",
       " 'Nan',\n",
       " 'Nancy',\n",
       " 'Naomi',\n",
       " 'Nara',\n",
       " 'Nat',\n",
       " 'Natalie',\n",
       " 'Nell',\n",
       " 'Nellie',\n",
       " 'Nina',\n",
       " 'Noel',\n",
       " 'Norma',\n",
       " 'Nova',\n",
       " 'Octavia',\n",
       " 'Odessa',\n",
       " 'Olga',\n",
       " 'Olive',\n",
       " 'Olivia',\n",
       " 'Oneida',\n",
       " 'Orly',\n",
       " 'Page',\n",
       " 'Pam',\n",
       " 'Pamela',\n",
       " 'Pandora',\n",
       " 'Pat',\n",
       " 'Patience',\n",
       " 'Patrice',\n",
       " 'Patricia',\n",
       " 'Patti',\n",
       " 'Patty',\n",
       " 'Paula',\n",
       " 'Pearl',\n",
       " 'Peg',\n",
       " 'Pen',\n",
       " 'Penny',\n",
       " 'Perle',\n",
       " 'Perry',\n",
       " 'Phil',\n",
       " 'Philippe',\n",
       " 'Philippine',\n",
       " 'Philly',\n",
       " 'Phyllis',\n",
       " 'Portia',\n",
       " 'Prisca',\n",
       " 'Prudence',\n",
       " 'Quintana',\n",
       " 'Rachel',\n",
       " 'Rae',\n",
       " 'Raine',\n",
       " 'Randy',\n",
       " 'Ray',\n",
       " 'Rebecca',\n",
       " 'Regina',\n",
       " 'Reine',\n",
       " 'Remy',\n",
       " 'Rey',\n",
       " 'Robbie',\n",
       " 'Robby',\n",
       " 'Roberta',\n",
       " 'Robin',\n",
       " 'Ronnie',\n",
       " 'Rosa',\n",
       " 'Rosabelle',\n",
       " 'Rosalie',\n",
       " 'Rose',\n",
       " 'Rosella',\n",
       " 'Rosemary',\n",
       " 'Rosie',\n",
       " 'Rosy',\n",
       " 'Row',\n",
       " 'Roxy',\n",
       " 'Rozella',\n",
       " 'Rozelle',\n",
       " 'Ruth',\n",
       " 'Saba',\n",
       " 'Sabina',\n",
       " 'Sabine',\n",
       " 'Sadie',\n",
       " 'Sally',\n",
       " 'Sam',\n",
       " 'Sammy',\n",
       " 'Sandra',\n",
       " 'Sara',\n",
       " 'Sarah',\n",
       " 'Scarlet',\n",
       " 'Scotty',\n",
       " 'Sean',\n",
       " 'Selena',\n",
       " 'Selma',\n",
       " 'Serene',\n",
       " 'Shannon',\n",
       " 'Shari',\n",
       " 'Sharon',\n",
       " 'Shawnee',\n",
       " 'Shay',\n",
       " 'Shayne',\n",
       " 'Shea',\n",
       " 'Sheila',\n",
       " 'Shelagh',\n",
       " 'Shelby',\n",
       " 'Shell',\n",
       " 'Shelley',\n",
       " 'Sherry',\n",
       " 'Shirl',\n",
       " 'Shirley',\n",
       " 'Sibley',\n",
       " 'Sibylla',\n",
       " 'Sioux',\n",
       " 'Sonny',\n",
       " 'Sophia',\n",
       " 'Sophie',\n",
       " 'Stacey',\n",
       " 'Stacy',\n",
       " 'Star',\n",
       " 'Starr',\n",
       " 'Stella',\n",
       " 'Stephanie',\n",
       " 'Stormy',\n",
       " 'Sue',\n",
       " 'Sunny',\n",
       " 'Sunshine',\n",
       " 'Susan',\n",
       " 'Susie',\n",
       " 'Suzanne',\n",
       " 'Sybil',\n",
       " 'Sydney',\n",
       " 'Sylvie',\n",
       " 'Tara',\n",
       " 'Tate',\n",
       " 'Ted',\n",
       " 'Teddy',\n",
       " 'Teresa',\n",
       " 'Terry',\n",
       " 'Tess',\n",
       " 'Tessie',\n",
       " 'Thea',\n",
       " 'Thelma',\n",
       " 'Theresa',\n",
       " 'Tillie',\n",
       " 'Tim',\n",
       " 'Timmy',\n",
       " 'Tommie',\n",
       " 'Tommy',\n",
       " 'Toni',\n",
       " 'Tony',\n",
       " 'Tootsie',\n",
       " 'Tory',\n",
       " 'Tuesday',\n",
       " 'Ursuline',\n",
       " 'Vale',\n",
       " 'Valentine',\n",
       " 'Valerie',\n",
       " 'Valery',\n",
       " 'Van',\n",
       " 'Ventura',\n",
       " 'Venus',\n",
       " 'Vera',\n",
       " 'Vere',\n",
       " 'Veronica',\n",
       " 'Vicky',\n",
       " 'Victoria',\n",
       " 'Vida',\n",
       " 'Viola',\n",
       " 'Violet',\n",
       " 'Virginia',\n",
       " 'Vita',\n",
       " 'Vivian',\n",
       " 'Wally',\n",
       " 'Whitney',\n",
       " 'Wilhelmina',\n",
       " 'Willa',\n",
       " 'Willie',\n",
       " 'Willow',\n",
       " 'Willy',\n",
       " 'Wilmette',\n",
       " 'Wynn',\n",
       " 'Xenia',\n",
       " 'Yvette',\n",
       " 'Zara',\n",
       " 'Zoe'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_names_in_brown = set(names.words(\"female.txt\")).intersection(brown.words())\n",
    "fem_names_in_brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (Programmatically) create a set of the words from `mini_sense_lexicon` which have an animate noun sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bear', 'slug'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animates = set()\n",
    "for word, pos_list in mini_sense_lexicon.items():\n",
    "    for feature_dict in pos_list:\n",
    "        if \"animate\" in feature_dict and feature_dict[\"animate\"]:\n",
    "            animates.add(word)\n",
    "animates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Count how often each English phone appears in CMU lexicon, striping off the stress markers at the end of vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'AH': 71410, 'EY': 13521, 'F': 13748, 'AO': 11290, 'R': 46046, 'T': 48549, 'UW': 9736, 'W': 8864, 'N': 60564, 'IH': 50093, 'P': 19715, 'L': 49479, 'AA': 24546, 'B': 21057, 'ER': 29027, 'G': 13553, 'K': 42502, 'S': 50427, 'EH': 27398, 'TH': 2902, 'M': 29347, 'D': 32389, 'V': 10742, 'Z': 27842, 'IY': 34504, 'AE': 21804, 'OW': 19047, 'NG': 9865, 'SH': 8700, 'HH': 9319, 'AW': 3408, 'AY': 11313, 'JH': 6404, 'Y': 5171, 'CH': 4960, 'ZH': 560, 'UH': 2273, 'DH': 576, 'OY': 1267})\n"
     ]
    }
   ],
   "source": [
    "phone_counts = defaultdict(int)\n",
    "for pronunciations in p_dict.values():\n",
    "    for pronunciation in pronunciations:\n",
    "        for phone in pronunciation:\n",
    "            if phone[-1].isdigit():\n",
    "                phone = phone[:-1]\n",
    "            phone_counts[phone] += 1\n",
    "            \n",
    "print(phone_counts)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
