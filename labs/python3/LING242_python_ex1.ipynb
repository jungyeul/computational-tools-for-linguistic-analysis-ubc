{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LING 242 Exercise 1: Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exerccise will\n",
    "- give you some more practice with the basic string methods available in Python\n",
    "- have you iterate over NLTK corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "This exerccise requires that you have downloaded following NLTK corpora/lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"treebank\")\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below so you can access them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank,brown,reuters,stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: POS-tagged sentence to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagged sentences from the Brown corpus accessed by iterating using the <i>tagged_sents</i> method look like this:\n",
    "\n",
    "\\[('The', 'AT'), ('Hartsfield', 'NP'), ('home', 'NR'), ('is', 'BEZ'), ('at', 'IN'), ('637', 'CD'), ('E.', 'NP'), ('Pelham', 'NP'), ('Rd.', 'NN-TL'), ('Aj', 'NN'), ('.', '.')\\]\n",
    "\n",
    "That is, they are lists of (word, part-of-speech) tuples, where both word and the POS are strings. You'll notice that the POS tags for words like \"Hartsfield\" and \"home\" start with \"N\", indicating these are nouns, see [the wikipedia entry for the Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used) for more infomation about the meanings of these tags.\n",
    "\n",
    "In this exercise you will convert a sentence of the Brown corpus to a single string (appropriate for writing to disk) that looks like this:\n",
    "\n",
    "the/AT hartsfield/NP home/NR is/BEZ at/IN 637/CD e./NP pelham/NP rd./NN Aj/NN ./.\n",
    "\n",
    "Note the following changes:\n",
    "- the sentence has been converted into a single string\n",
    "- the words are now lowercase\n",
    "- any POS tag with a hyphen consists only of the part before the hyphen (the marking after the hyphen contains information that is not included in most POS tagsets)\n",
    "- Each word and part of speech tag is separated by a forward slash (/)\n",
    "- Each word/pos pair is separated by a space.\n",
    "\n",
    "Do this for the first sentence of the Brown corpus, which has already been extracted for you. Assign the result to the variable `string_sent` and check you have done this correctly using the provided asserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sent = brown.tagged_sents()[0]\n",
    "string_sent = []\n",
    "\n",
    "# print(string_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert string_sent == \"the/AT fulton/NP county/NN grand/JJ jury/NN said/VBD friday/NR an/AT investigation/NN of/IN atlanta's/NP$ recent/JJ primary/NN election/NN produced/VBD ``/`` no/AT evidence/NN ''/'' that/CS any/DTI irregularities/NNS took/VBD place/NN ./.\"\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Exploring the Reuters corpus\n",
    "\n",
    "NLTK provides access to the `reuters` corpus, which are a collection of Reuters new articles labelled by topic. \n",
    "\n",
    "* The size of the corpus, in word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The number of word types, after lowercasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for word in reuters.words():\n",
    "    word_list.append(word.lower())\n",
    "\n",
    "len(Counter(set(word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The topics associated with the document labelled 'test/14840':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The number of documents which are labelled with the \"gold\" topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for f in reuters.fileids():\n",
    "    if 'gold' in reuters.categories(f):\n",
    "        i += 1\n",
    "print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The total number of sentences in documents which have the \"tea\" topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for f in reuters.fileids('tea'):\n",
    "    num += len(reuters.sents(f))\n",
    "\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a word cloud for the \"coffee\" topic, excluding stopwords (this will take more than one line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = stopwords.words(\"English\")\n",
    "\n",
    "# wordcloud = WordCloud(stopwords = stopwords_set).generate(\" \".join(corpus.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Building word lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll be creating lists of words from the Brown corpus which have some particular property. You should iterate over each word in the corpus (using a <i>for</i> loop and the <i>words</i> method), and add to it the list (using append) if it is the kind of word you're looking for. Then you should print out the length of the list as well as the first five examples you found (remember that slicing also works for lists!). It is okay if you repeat the code for iterating over the Brown in each cell (also okay if you decompose this and use a function!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1\n",
    "\n",
    "Find words which contain the lowercase letter x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(word_list) == 9365\n",
    "assert \"index\" in word_list\n",
    "assert \"indices\" not in word_list\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "\n",
    "Find capitalized words (words that start with a upper case letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "#your code here\n",
    "# ...isupper():\n",
    "\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(word_list) == 125369\n",
    "assert \"The\" in word_list\n",
    "assert \"the\" not in word_list\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Find present particles (words of at least length 6 that end with \"ing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "#your code here\n",
    "# word[-3:] == 'ing' ...\n",
    "# endswith('ing') ...\n",
    "\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(word_list) == 25038\n",
    "assert \"waiting\" in word_list\n",
    "assert \"wait\" not in word_list\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Find entirely alphabetic words which contain no vowel. Your solution should be case insensitive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = set([\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"])\n",
    "word_list = []\n",
    "#your code here\n",
    "\n",
    "print(len(word_list))\n",
    "print(word_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(word_list) == 1098\n",
    "assert \"CD\" in word_list\n",
    "assert \"CAD\" not in word_list\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5\n",
    "\n",
    "Find all words which contain a single hyphen and each of the parts of the hypenated word is at least four letters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "#your code here\n",
    "#  word.find('-') == word.rfind('-') ## rfind() = last occurrence of the string;\n",
    "#  word.split(\"-\") == 2\n",
    "\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(word_list) == 4509\n",
    "assert \"hard-fought\" in word_list\n",
    "assert \"hard-won\" not in word_list\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Add one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numbers that you run across in text corpora will actually be strings, not integers or floats. If you want to use them as numbers for some purpose, you'll need to convert them. Iterate over the sentences in the Penn Treebank corpus (using the <i>sents</i> method) and find integers written in arabic numerials (your code doesn't need to handle numbers written in English, e.g. \"three\", though if you want to add that functionality, great!). When you find a number in a sentence, print the sentence, add one to the number, then print out the sentence again with the new number (HINT: you will need two loops, one which iterates over the sentences in the corpus and one that iterates over the words in each sentence; however, in order to modify the sentence as we have asked you to, you'll want to iterate over the words in the sentence using a index). It's okay if you print out the sentence multiple times if it contains multiple numbers.\n",
    "\n",
    "Both original and modified sentence should be lists of strings; to confirm this, please <i>join</i> them (with spaces) before you print them.  There will be some weird cases of numbers where you don't expect them: any idea what's going on there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sent:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for word in sent:\n",
    "    print(i, word)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sent)):\n",
    "    word = sent[i]\n",
    "    print(i, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(sent):\n",
    "    print(i, word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
