{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302c02e1-6020-4c9b-9292-c83cf915b858",
   "metadata": {},
   "source": [
    "# Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7d054-575f-41fb-96e8-1118e2785521",
   "metadata": {},
   "source": [
    "#### Language models seek to mathematically predict the probability of words occurring consecutively. This can be used to increase accuracy of predictive text. Strings of words of *n* size are known as n-grams, and language models can often be adjusted for different values of *n*. This lab will demonstrate some basic capabilities of language models, using two language modelling toolkits: KenLM (2011) and SRILM (2002). The focus of the lab will be on estimating probabilities and on evaluating perplexity (perplexity being a metric for how accurately a model calculates probabilities, with lower values being favourable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43691f4-6545-44bf-b23c-b9f44d71c09c",
   "metadata": {},
   "source": [
    "## 1. Mikolov's data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf50bd-0976-449d-93fc-a932f2d773ea",
   "metadata": {},
   "source": [
    "#### This first step of the lab is purely setup for the next two steps — all that must be done here is ensuring that the necessary data is downloaded and in the correct location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff0536-a927-4af8-9235-cd6b23f72fc8",
   "metadata": {},
   "source": [
    "##### Download the dataset. This data has been preprocessed from the Penn Treebank (PTB) by Mikolov (2012), with out-of-vocabulary (OOV) words being given a special label \\<unk\\>.\n",
    "\n",
    "##### What that accomplishes is preventing evaluations of things like perplexity from getting too caught up on rare words that might appear only once in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rolled-support",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-02 15:52:45--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Résolution de www.fit.vutbr.cz (www.fit.vutbr.cz)… 147.229.9.23\n",
      "Connexion à www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 34869662 (33M) [application/x-gtar]\n",
      "Sauvegarde en : « simple-examples.tgz »\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33,25M  5,75MB/s    ds 7,9s    \n",
      "\n",
      "2021-10-02 15:52:55 (4,20 MB/s) — « simple-examples.tgz » sauvegardé [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ceb94-842a-4a50-9555-5146b6f90218",
   "metadata": {},
   "source": [
    "##### Unzip the compressed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "horizontal-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xfz simple-examples.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213c887-46c4-4de0-987d-5f4ef00eeb24",
   "metadata": {},
   "source": [
    "##### List all contents of the active directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generous-problem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 84168\n",
      "drwxr-xr-x   3 marcellmaitinsky  staff        96  2 oct 00:42 \u001b[34mdata\u001b[m\u001b[m\n",
      "-rw-r--r--   1 marcellmaitinsky  staff    142573  1 oct 00:22 en_ewt-ud-dev-alt.txt\n",
      "-rw-r--r--   1 marcellmaitinsky  staff    126870  2 oct 00:41 en_ewt-ud-dev.txt\n",
      "-rw-r--r--   1 marcellmaitinsky  staff    126137  2 oct 09:48 en_ewt-ud-dev.txt.sbd_splitta\n",
      "drwxr-xr-x   3 marcellmaitinsky  staff        96 22 sep 18:55 \u001b[34mlab1\u001b[m\u001b[m\n",
      "-rw-r--r--   1 marcellmaitinsky  staff     23802 22 sep 18:55 lab1.ipynb\n",
      "-rw-r--r--   1 marcellmaitinsky  staff     21477  2 oct 09:50 lab2.ipynb\n",
      "drwxr-xr-x   2 marcellmaitinsky  staff        64  2 oct 15:52 \u001b[34mlab3\u001b[m\u001b[m\n",
      "-rw-r--r--   1 marcellmaitinsky  staff     27896  2 oct 15:52 lab3.ipynb\n",
      "drwxr-xr-x   3 marcellmaitinsky  staff        96 29 sep 18:01 \u001b[34mlab4\u001b[m\u001b[m\n",
      "drwxr-xr-x   8 marcellmaitinsky  staff       256  1 oct 23:26 \u001b[34mscripts\u001b[m\u001b[m\n",
      "-rw-r--r--   1 marcellmaitinsky  staff      4927 29 déc  2007 scripts.tgz\n",
      "drwx------  15 marcellmaitinsky  staff       480  2 mai  2011 \u001b[34msimple-examples\u001b[m\u001b[m\n",
      "-rw-r--r--   1 marcellmaitinsky  staff  34869662 12 sep  2011 simple-examples.tgz\n",
      "drwxr-xr-x  12 marcellmaitinsky  staff       384  2 oct 10:14 \u001b[34msplitta-0.1.0\u001b[m\u001b[m\n",
      "-rw-r--r--   1 marcellmaitinsky  staff   6943603 26 fév  2020 splitta-0.1.0.tar.gz\n",
      "-rwxr-xr-x@  1 marcellmaitinsky  staff      2208  1 oct 16:27 \u001b[31mtokenizer.sed\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3357fcb-c922-4e0d-bc82-86a2a7873847",
   "metadata": {},
   "source": [
    "##### Change directory to enter the folder containing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continent-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/simple-examples\n"
     ]
    }
   ],
   "source": [
    "%cd simple-examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fff3af-edf3-4e8e-9bbb-10985e9096b6",
   "metadata": {},
   "source": [
    "##### List contents of active directory, as well as the contents of those aforementioned contents if applicable (this is what -R as opposed to -l does after ls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "played-grade",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m1-train\u001b[m\u001b[m                    \u001b[34m8-direct\u001b[m\u001b[m\n",
      "\u001b[34m2-nbest-rescore\u001b[m\u001b[m            \u001b[34m9-char-based-lm\u001b[m\u001b[m\n",
      "\u001b[34m3-combination\u001b[m\u001b[m              \u001b[34mdata\u001b[m\u001b[m\n",
      "\u001b[34m4-data-generation\u001b[m\u001b[m          \u001b[34mmodels\u001b[m\u001b[m\n",
      "\u001b[34m5-one-iter\u001b[m\u001b[m                 \u001b[34mrnnlm-0.2b\u001b[m\u001b[m\n",
      "\u001b[34m6-recovery-during-training\u001b[m\u001b[m \u001b[34mtemp\u001b[m\u001b[m\n",
      "\u001b[34m7-dynamic-evaluation\u001b[m\u001b[m\n",
      "\n",
      "./1-train:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./2-nbest-rescore:\n",
      "\u001b[31mREADME\u001b[m\u001b[m      getbest.c   gettext.c   \u001b[31mmakenbest\u001b[m\u001b[m\n",
      "\u001b[31mgetbest\u001b[m\u001b[m     \u001b[31mgettext\u001b[m\u001b[m     \u001b[34mlattices\u001b[m\u001b[m    makenbest.c\n",
      "\n",
      "./2-nbest-rescore/lattices:\n",
      "AMI-3E0501_u3005_127040_127488.lat.gz AMI-3E0501_u3005_128490_129032.lat.gz\n",
      "AMI-3E0501_u3005_127513_127835.lat.gz latlist\n",
      "AMI-3E0501_u3005_127865_128175.lat.gz \u001b[34mnbest\u001b[m\u001b[m\n",
      "AMI-3E0501_u3005_128188_128447.lat.gz \u001b[31mnbest.sh\u001b[m\u001b[m\n",
      "\n",
      "./2-nbest-rescore/lattices/nbest:\n",
      "\n",
      "./3-combination:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./4-data-generation:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./5-one-iter:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./6-recovery-during-training:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./7-dynamic-evaluation:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./8-direct:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./9-char-based-lm:\n",
      "README   \u001b[31mtest.sh\u001b[m\u001b[m  \u001b[31mtrain.sh\u001b[m\u001b[m\n",
      "\n",
      "./data:\n",
      "README             ptb.char.train.txt ptb.test.txt       ptb.valid.txt\n",
      "ptb.char.test.txt  ptb.char.valid.txt ptb.train.txt\n",
      "\n",
      "./models:\n",
      "README          swb.ngram.model swb.rnn.model\n",
      "\n",
      "./rnnlm-0.2b:\n",
      "CHANGE.log     convert.c      makefile       rnnlmlib.cpp   train\n",
      "COPYRIGHT.txt  example.output prob.c         rnnlmlib.h     valid\n",
      "FAQ.txt        \u001b[31mexample.sh\u001b[m\u001b[m     rnnlm.cpp      test\n",
      "\n",
      "./temp:\n"
     ]
    }
   ],
   "source": [
    "!ls -R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1207db-e81a-4f0a-b6e3-72ec02860d93",
   "metadata": {},
   "source": [
    "##### Change to parent directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fancy-nylon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-tamil",
   "metadata": {},
   "source": [
    "## 2. KenLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad16604-3181-491d-a812-86be0b2238bd",
   "metadata": {},
   "source": [
    "#### KenLM is software that was developed for the purpose of executing various mathematical operations on language models. This section of the lab will give an introduction to KenLM's estimation and querying capabilities in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c989eae-7b90-4905-bbfc-b613ebcc0e13",
   "metadata": {},
   "source": [
    "##### Download kenlm and then unpack it, in a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enormous-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-02 15:54:24--  https://kheafield.com/code/kenlm.tar.gz\n",
      "Résolution de kheafield.com (kheafield.com)… 35.196.63.85\n",
      "Connexion à kheafield.com (kheafield.com)|35.196.63.85|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 491090 (480K) [application/x-gzip]\n",
      "Sauvegarde en : « STDOUT »\n",
      "\n",
      "-                   100%[===================>] 479,58K  1,15MB/s    ds 0,4s    \n",
      "\n",
      "2021-10-02 15:54:25 (1,15 MB/s) — envoi vers sortie standard [491090/491090]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O - https://kheafield.com/code/kenlm.tar.gz  | tar xz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49404564-29f4-4358-bf19-9deb79f33836",
   "metadata": {},
   "source": [
    "##### Create a folder within your existing kenlm folder. This is where you will compile KenLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tropical-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir kenlm/build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88cb70-894a-4716-9718-3de402f16b5c",
   "metadata": {},
   "source": [
    "##### Change directory to make sure you are in your newly-created \"build\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handy-poster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/kenlm/build\n"
     ]
    }
   ],
   "source": [
    "%cd kenlm/build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c7863-06cb-4337-aad5-147845d7f57c",
   "metadata": {},
   "source": [
    "##### Compile KenLM into the build folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broken-concert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
      "-- Found Boost: /usr/local/lib/cmake/Boost-1.76.0/BoostConfig.cmake (found suitable version \"1.76.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
      "-- Found ZLIB: /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/libz.tbd (found version \"1.2.11\") \n",
      "-- Found BZip2: /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/libbz2.tbd (found version \"1.0.6\") \n",
      "-- Looking for BZ2_bzCompressInit\n",
      "-- Looking for BZ2_bzCompressInit - found\n",
      "-- Looking for lzma_auto_decoder in /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/liblzma.tbd\n",
      "-- Looking for lzma_auto_decoder in /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/liblzma.tbd - found\n",
      "-- Looking for lzma_easy_encoder in /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/liblzma.tbd\n",
      "-- Looking for lzma_easy_encoder in /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/liblzma.tbd - found\n",
      "-- Looking for lzma_lzma_preset in /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/liblzma.tbd\n",
      "-- Looking for lzma_lzma_preset in /Library/Developer/CommandLineTools/SDKs/MacOSX11.3.sdk/usr/lib/liblzma.tbd - found\n",
      "-- Could NOT find LibLZMA (missing: LIBLZMA_INCLUDE_DIR) \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/kenlm/build\n"
     ]
    }
   ],
   "source": [
    "!cmake .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1adb1-b252-40d7-b650-49665f90a845",
   "metadata": {},
   "source": [
    "##### Notice how some files could not be found. Normally \"cmake\" is preferred over \"make\" because it is equally usable on multiple operating systems, including the most popular Windows and Mac OS, but since cmake has not been able to fully compile KenLM, make is necessary too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "painted-observer",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
      "[ 38%] Built target kenlm_util\n",
      "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
      "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
      "[ 41%] Built target probing_hash_table_benchmark\n",
      "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
      "In file included from /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/kenlm/lm/vocab.cc:11:\n",
      "\u001b[1m/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/kenlm/util/joint_sort.hh:60:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1manonymous non-C-compatible type given name for linkage purposes by typedef declaration; add a tag name here [-Wnon-c-typedef-for-linkage]\u001b[0m\n",
      "    typedef struct {\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[0;32m                   value_type\n",
      "\u001b[0m\u001b[1m/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/kenlm/util/joint_sort.hh:63:7: \u001b[0m\u001b[0;1;30mnote: \u001b[0mtype is not C-compatible due to this member declaration\u001b[0m\n",
      "      const typename std::iterator_traits<KeyIter>::value_type &GetKey() const { return key; }\n",
      "\u001b[0;1;32m      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\u001b[0m\u001b[1m/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/kenlm/util/joint_sort.hh:64:7: \u001b[0m\u001b[0;1;30mnote: \u001b[0mtype is given name 'value_type' for linkage purposes by this typedef declaration\u001b[0m\n",
      "    } value_type;\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m1 warning generated.\n",
      "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
      "[ 66%] Built target kenlm\n",
      "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
      "[ 68%] Built target query\n",
      "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
      "[ 71%] Built target kenlm_benchmark\n",
      "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
      "[ 73%] Built target build_binary\n",
      "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
      "[ 76%] Built target fragment\n",
      "[ 77%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
      "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
      "[ 85%] Built target kenlm_builder\n",
      "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
      "[ 87%] Built target count_ngrams\n",
      "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
      "[ 90%] Built target lmplz\n",
      "[ 91%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
      "[ 95%] Built target kenlm_filter\n",
      "[ 96%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
      "[ 97%] Built target phrase_table_vocab\n",
      "[ 98%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
      "[100%] Built target filter\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76580fa-94ed-464d-ab23-89419fb67885",
   "metadata": {},
   "source": [
    "##### Output the first 10 lines (no exact number specified, so default is 10) of ptb.train.txt to a text file. Also make sure the current standard \"\\<unk\\>\" label for OOV words is replaced in all instances with just a capital \"UNK\" label in the new text file, using sed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "soviet-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ~/LING242/computational-tools-for-linguistic-analysis-ubc/labs/simple-examples/data/ptb.train.txt | sed 's/<unk>/UNK/g' > text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a8bf2-679b-4dc5-a79d-7ac4e113a5e2",
   "metadata": {},
   "source": [
    "##### Estimate ngram probabilities using lmplz, with \"-o 5\" specifying the maximum value of n you want to calculate n-gram probabilities for. So here you are calculating for unigrams, 2-grams, 3-grams, 4-grams, and 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "under-tissue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading stdin\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 213 types 143\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1716 2:1340867584 3:2514126848 4:4022602752 5:5866296320\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/kenlm/lm/builder/adjust_counts.cc:52 in void lm::builder::(anonymous namespace)::StatCollector::CalculateDiscounts(const lm::builder::DiscountConfig &) threw BadDiscountException because `s.n[j] == 0'.\n",
      "Could not calculate Kneser-Ney discounts for 2-grams with adjusted count 4 because we didn't observe any 2-grams with adjusted count 3; Is this small or artificial data?\n",
      "Try deduplicating the input.  To override this error for e.g. a class-based model, rerun with --discount_fallback\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bin/lmplz -o 5 <text >text.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740db21e-bbb3-4d1b-9bd7-0b2837d42ced",
   "metadata": {},
   "source": [
    "##### Note \"Is this small or artificial data?\" — in this case the data is indeed small, which is why aiming for 5-grams does not work. Reducing it down to 2 is more feasible for this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "impossible-falls",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading stdin\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 213 types 143\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1716 2:13743893504\n",
      "Statistics:\n",
      "1 143 D1=0.771812 D2=1.7276 D3+=1.45638\n",
      "2 208 D1=0.881818 D2=1.7965 D3+=3\n",
      "Memory estimate for binary LM:\n",
      "type       B\n",
      "probing 7472 assuming -p 1.5\n",
      "probing 8048 assuming -r models -p 1.5\n",
      "trie    4499 without quantization\n",
      "trie    4930 assuming -q 8 -b 8 quantization \n",
      "trie    4499 assuming -a 22 array pointer compression\n",
      "trie    4930 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:1716 2:3328\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:1716 2:3328\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "RSSMax:2765594624 kB\tuser:0.801218\tsys:1.58579\tCPU:2.38703\treal:2.71814\n"
     ]
    }
   ],
   "source": [
    "!bin/lmplz -o 2 <text >text.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b32b9-9ad8-4801-8ab9-a625842b1edc",
   "metadata": {},
   "source": [
    "##### The command used above has taken data from the file \"text\", calculated probabilities, and generated a language model in \"text.arpa\". It is not shown here, but arpa files store log probabilities rather than exact probabilities, to avoid having excessively long decimals for n-grams with low probabilities. This often results in what looks like negative probabilities, but those are simply the exponent of 10 needed to get the actual probability in question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f5b49-9c39-44ba-a847-49ea2d02b20b",
   "metadata": {},
   "source": [
    "##### Convert the language model to binary to make querying more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parliamentary-omega",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!bin/build_binary text.arpa text.binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27def9cd-8100-430d-91ae-c27fa9fce06e",
   "metadata": {},
   "source": [
    "##### Feed the end of the ptb.train.txt file into a new file called \"data\", once again making sure to replace \\<unk\\> with \"UNK\" to ensure querying works correctly. Unlike the head command from earlier, this tail command does have a number specified, and in this case, to ensure speed, this number is 1. That is, only the very last line of the data is actually being used. Ideally this would not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fossil-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 1 /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/simple-examples/data/ptb.train.txt   | sed 's/<unk>/UNK/g' > data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398ee0cf-18c7-4446-a3e9-53b7187575d3",
   "metadata": {},
   "source": [
    "##### Complete the querying for the binary version of the language model, using the data file from the previous step to help calculate perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "healthy-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This binary file contains probing hash tables.\n",
      "in=110 1 -1.9787562\tlos=0 1 -2.4069233\tangeles=0 1 -2.3523023\tfor=0 1 -2.3523023\texample=0 1 -2.3523023\tcentral=0 1 -2.3523023\thas=71 1 -2.2564688\thad=0 1 -2.4069233\ta=37 1 -1.5073059\tstrong=0 1 -2.4049046\tmarket=0 1 -2.3523023\tposition=0 1 -2.3523023\twhile=0 1 -2.3523023\tunilab=0 1 -2.3523023\t's=121 1 -2.2564688\tpresence=0 1 -2.4069233\thas=71 1 -2.2564688\tbeen=0 1 -2.4069233\tless=0 1 -2.3523023\tprominent=0 1 -2.3523023\taccording=0 1 -2.3523023\tto=66 1 -1.6679683\tmr.=41 1 -2.3078644\tUNK=28 2 -0.76690423\t</s>=2 1 -1.4424298\tTotal: -54.348557 OOV: 16\n",
      "Perplexity including OOVs:\t149.25959564199744\n",
      "Perplexity excluding OOVs:\t67.10224231949275\n",
      "OOVs:\t16\n",
      "Tokens:\t25\n",
      "RSSMax:1220608 kB\tuser:0.005163\tsys:0.009904\tCPU:0.015094\treal:0.000893\n"
     ]
    }
   ],
   "source": [
    "!bin/query text.binary <data                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2efb0-b1da-4998-87f9-55ba40d979fd",
   "metadata": {},
   "source": [
    "##### The output here shows the n-gram itself (in this case all unigrams), with an '=' followed by ???, then the n-gram length and the log probability. As explained earlier, the probabilities are negative because they are the exponent of 10 needed to find the actual probability. The output also includes perplexity (notice that the perplexity is lower when OOVs are excluded, showing higher accuracy), as well as number of OOVs and total number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e26b70-709a-4676-aab4-9ae6c97d84da",
   "metadata": {},
   "source": [
    "## 3. SRILM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8445e00-c4c3-4ea6-854d-5560f35e6b0a",
   "metadata": {},
   "source": [
    "##### SRILM is yet another language modelling software, somewhat older than KenLM. Both softwares have similar applications, such as estimating probabilities and querying. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41d676-fa86-4647-9af0-91f70c87ed75",
   "metadata": {},
   "source": [
    "**TODO** complete commands for _srilm_ in jupyter except for downloading. It should include\n",
    "* compile, \n",
    "* estimate probabilities, and\n",
    "* query \n",
    "\n",
    "Use `ptb.train.txt` for training (estimating) and `ptb.test.txt` for querying to calculate perplexities. \n",
    "When you estimate probabilities, we use the standard LM option. See [[link1](https://www.statmt.org/wmt09/baseline.html)] and [[link2](https://kheafield.com/code/kenlm/estimation/)]. For querying, see [[link](http://www.speech.sri.com/projects/srilm/manpages/ngram-count.1.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53ad22-8af1-44e7-aeda-9b30cb48ed5e",
   "metadata": {},
   "source": [
    "##### Change into the correct directory for srilm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf905057-6d42-49c0-b7bc-2e737c918ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2070d7-0827-4ea4-b02a-b04e062f7b42",
   "metadata": {},
   "source": [
    "##### Compile srilm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "726d015a-97ad-4885-bb25-c26ae6a70b82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p include lib bin\n",
      "/Library/Developer/CommandLineTools/usr/bin/make init\n",
      "for subdir in misc dstruct lm flm lattice utils zlib; do \\\n",
      "\t\t(cd $subdir/src; /Library/Developer/CommandLineTools/usr/bin/make SRILM=/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm MACHINE_TYPE=macosx OPTION= MAKE_PIC= init) || exit 1; \\\n",
      "\tdone\n",
      "cd ..; /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/make-standard-directories\n",
      "/Library/Developer/CommandLineTools/usr/bin/make ../obj/macosx/STAMP ../bin/macosx/STAMP\n",
      "make[3]: `../obj/macosx/STAMP' is up to date.\n",
      "make[3]: `../bin/macosx/STAMP' is up to date.\n",
      "cd ..; /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/make-standard-directories\n",
      "/Library/Developer/CommandLineTools/usr/bin/make ../obj/macosx/STAMP ../bin/macosx/STAMP\n",
      "make[3]: `../obj/macosx/STAMP' is up to date.\n",
      "make[3]: `../bin/macosx/STAMP' is up to date.\n",
      "cd ..; /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/make-standard-directories\n",
      "/Library/Developer/CommandLineTools/usr/bin/make ../obj/macosx/STAMP ../bin/macosx/STAMP\n",
      "make[3]: `../obj/macosx/STAMP' is up to date.\n",
      "make[3]: `../bin/macosx/STAMP' is up to date.\n",
      "cd ..; /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/make-standard-directories\n",
      "/Library/Developer/CommandLineTools/usr/bin/make ../obj/macosx/STAMP ../bin/macosx/STAMP\n",
      "make[3]: `../obj/macosx/STAMP' is up to date.\n",
      "make[3]: `../bin/macosx/STAMP' is up to date.\n",
      "cd ..; /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/make-standard-directories\n",
      "/Library/Developer/CommandLineTools/usr/bin/make ../obj/macosx/STAMP ../bin/macosx/STAMP\n",
      "make[3]: `../obj/macosx/STAMP' is up to date.\n",
      "make[3]: `../bin/macosx/STAMP' is up to date.\n",
      "cd ..; /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/make-standard-directories\n",
      "/Library/Developer/CommandLineTools/usr/bin/make ../obj/macosx/STAMP ../bin/macosx/STAMP\n",
      "make[3]: `../obj/macosx/STAMP' is up to date.\n",
      "make[3]: `../bin/macosx/STAMP' is up to date.\n",
      "cd ..; /Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/make-standard-directories\n",
      "/Library/Developer/CommandLineTools/usr/bin/make ../obj/macosx/STAMP ../bin/macosx/STAMP\n",
      "make[3]: `../obj/macosx/STAMP' is up to date.\n",
      "make[3]: `../bin/macosx/STAMP' is up to date.\n",
      "/Library/Developer/CommandLineTools/usr/bin/make release-headers\n",
      "for subdir in misc dstruct lm flm lattice utils zlib; do \\\n",
      "\t\t(cd $subdir/src; /Library/Developer/CommandLineTools/usr/bin/make SRILM=/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm MACHINE_TYPE=macosx OPTION= MAKE_PIC= release-headers) || exit 1; \\\n",
      "\tdone\n",
      "make[2]: Nothing to be done for `release-headers'.\n",
      "make[2]: Nothing to be done for `release-headers'.\n",
      "make[2]: Nothing to be done for `release-headers'.\n",
      "make[2]: Nothing to be done for `release-headers'.\n",
      "make[2]: Nothing to be done for `release-headers'.\n",
      "make[2]: Nothing to be done for `release-headers'.\n",
      "make[2]: Nothing to be done for `release-headers'.\n",
      "/Library/Developer/CommandLineTools/usr/bin/make depend\n",
      "for subdir in misc dstruct lm flm lattice utils zlib; do \\\n",
      "\t\t(cd $subdir/src; /Library/Developer/CommandLineTools/usr/bin/make SRILM=/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm MACHINE_TYPE=macosx OPTION= MAKE_PIC= depend) || exit 1; \\\n",
      "\tdone\n",
      "rm -f Dependencies.macosx\n",
      "cc -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual    -I../obj/macosx -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./option.c ./zio.c ./fcheck.c ./rand48.c ./version.c ./ztest.c | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "c++ -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual -DINSTANTIATE_TEMPLATES   -I../obj/macosx -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./Debug.cc ./File.cc ./MStringTokUtil.cc ./tls.cc ./tserror.cc ./tclmain.cc ./testFile.cc ./testRand.cc | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/generate-program-dependencies ../bin/macosx ../obj/macosx \"\" ztest testFile testRand | sed -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "rm -f Dependencies.macosx\n",
      "cc -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual    -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./qsort.c ./maxalloc.c | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "c++ -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual -DINSTANTIATE_TEMPLATES   -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./MemStats.cc ./LHashTrie.cc ./SArrayTrie.cc ./BlockMalloc.cc ./DStructThreads.cc ./Array.cc ./IntervalHeap.cc ./Map.cc ./SArray.cc ./LHash.cc ./Map2.cc ./Trie.cc ./CachedMem.cc ./testArray.cc ./testMap.cc ./testFloatMap.cc ./benchHash.cc ./testHash.cc ./testSizes.cc ./testCachedMem.cc ./testBlockMalloc.cc ./testMap2.cc ./testTrie.cc | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/generate-program-dependencies ../bin/macosx ../obj/macosx \"\" maxalloc testArray testMap testFloatMap benchHash testHash testSizes testCachedMem testBlockMalloc testMap2 testTrie | sed -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "rm -f Dependencies.macosx\n",
      "cc -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual    -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./matherr.c | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "c++ -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual -DINSTANTIATE_TEMPLATES   -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./Prob.cc ./Counts.cc ./XCount.cc ./Vocab.cc ./VocabMap.cc ./VocabMultiMap.cc ./VocabDistance.cc ./SubVocab.cc ./MultiwordVocab.cc ./TextStats.cc ./LM.cc ./LMClient.cc ./LMStats.cc ./RefList.cc ./Bleu.cc ./NBest.cc ./NBestSet.cc ./NgramLM.cc ./NgramStatsInt.cc ./NgramStatsShort.cc ./NgramStatsLong.cc ./NgramStatsLongLong.cc ./NgramStatsFloat.cc ./NgramStatsDouble.cc ./NgramStatsXCount.cc ./NgramProbArrayTrie.cc ./NgramCountLM.cc ./MSWebNgramLM.cc ./Discount.cc ./ClassNgram.cc ./SimpleClassNgram.cc ./DFNgram.cc ./SkipNgram.cc ./HiddenNgram.cc ./HiddenSNgram.cc ./VarNgram.cc ./DecipherNgram.cc ./TaggedVocab.cc ./TaggedNgram.cc ./TaggedNgramStats.cc ./StopNgram.cc ./StopNgramStats.cc ./MultiwordLM.cc ./NonzeroLM.cc ./BayesMix.cc ./LoglinearMix.cc ./AdaptiveMix.cc ./AdaptiveMarginals.cc ./CacheLM.cc ./DynamicLM.cc ./HMMofNgrams.cc ./WordAlign.cc ./WordLattice.cc ./WordMesh.cc ./simpleTrigram.cc ./LMThreads.cc ./MEModel.cc ./hmaxent.cc ./NgramStats.cc ./Trellis.cc ./testBinaryCounts.cc ./testHash.cc ./testProb.cc ./testQuantized.cc ./testXCount.cc ./testParseFloat.cc ./testVocabDistance.cc ./testNgram.cc ./testNgramAlloc.cc ./testNgramProbArrayTrie.cc ./testMultiReadLM.cc ./hoeffding.cc ./tolower.cc ./testLattice.cc ./testError.cc ./testNBest.cc ./testMix.cc ./testTaggedVocab.cc ./testVocab.cc ./ngram.cc ./ngram-count.cc ./ngram-merge.cc ./ngram-class.cc ./disambig.cc ./anti-ngram.cc ./nbest-lattice.cc ./nbest-mix.cc ./nbest-optimize.cc ./nbest-pron-score.cc ./segment.cc ./segment-nbest.cc ./hidden-ngram.cc ./multi-ngram.cc | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/generate-program-dependencies ../bin/macosx ../obj/macosx \"\" testBinaryCounts testHash testProb testQuantized testXCount testParseFloat testVocabDistance testNgram testNgramAlloc testNgramProbArrayTrie testMultiReadLM hoeffding tolower testLattice testError testNBest testMix testTaggedVocab testVocab  ngram ngram-count ngram-merge ngram-class disambig anti-ngram nbest-lattice nbest-mix nbest-optimize nbest-pron-score segment segment-nbest hidden-ngram multi-ngram | sed -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "rm -f Dependencies.macosx\n",
      "c++ -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual -DINSTANTIATE_TEMPLATES   -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./FDiscount.cc ./FNgramStats.cc ./FNgramStatsInt.cc ./FNgramSpecs.cc ./FNgramSpecsInt.cc ./FactoredVocab.cc ./FNgramLM.cc ./ProductVocab.cc ./ProductNgram.cc ./FLMThreads.cc ./strtolplusb.cc ./wmatrix.cc ./pngram.cc ./fngram-count.cc ./fngram.cc | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/generate-program-dependencies ../bin/macosx ../obj/macosx \"\" pngram fngram-count fngram  | sed -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "rm -f Dependencies.macosx\n",
      "c++ -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual -DINSTANTIATE_TEMPLATES   -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./Lattice.cc ./LatticeAlign.cc ./LatticeExpand.cc ./LatticeIndex.cc ./LatticeNBest.cc ./LatticeNgrams.cc ./LatticeReduce.cc ./HTKLattice.cc ./LatticeLM.cc ./LatticeThreads.cc ./LatticeDecode.cc ./testLattice.cc ./lattice-tool.cc | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/generate-program-dependencies ../bin/macosx ../obj/macosx \"\" testLattice  lattice-tool | sed -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "rm -f Dependencies.macosx\n",
      "c++ -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual -DINSTANTIATE_TEMPLATES   -I. -I../../include -DHAVE_ZOPEN -MM  ./nbest-rover-helper.cc | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/generate-program-dependencies ../bin/macosx ../obj/macosx \"\" nbest-rover-helper  | sed -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "rm -f Dependencies.macosx\n",
      "cc -Wall -Wno-unused-variable -Wno-uninitialized -Wno-overloaded-virtual    -I/usr/include -I. -I../../include -DHAVE_ZOPEN -MM  ./adler32.c ./crc32.c ./gzclose.c ./gzread.c ./infback.c ./inflate.c ./trees.c ./zutil.c ./compress.c ./deflate.c ./gzlib.c ./gzwrite.c ./inffast.c ./inftrees.c ./uncompr.c ./minigzip.c | sed -e \"s&^\\([^ ]\\)&../obj/macosx\"'$(OBJ_OPTION)'\"/\\1&g\" -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm/sbin/generate-program-dependencies ../bin/macosx ../obj/macosx \"\" minigzip | sed -e \"s&\\.o&.o&g\" >> Dependencies.macosx\n",
      "/Library/Developer/CommandLineTools/usr/bin/make release-libraries\n",
      "for subdir in misc dstruct lm flm lattice utils zlib; do \\\n",
      "\t\t(cd $subdir/src; /Library/Developer/CommandLineTools/usr/bin/make SRILM=/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm MACHINE_TYPE=macosx OPTION= MAKE_PIC= release-libraries) || exit 1; \\\n",
      "\tdone\n",
      "make[2]: Nothing to be done for `release-libraries'.\n",
      "make[2]: Nothing to be done for `release-libraries'.\n",
      "make[2]: Nothing to be done for `release-libraries'.\n",
      "make[2]: Nothing to be done for `release-libraries'.\n",
      "make[2]: Nothing to be done for `release-libraries'.\n",
      "make[2]: Nothing to be done for `release-libraries'.\n",
      "make[2]: Nothing to be done for `release-libraries'.\n",
      "/Library/Developer/CommandLineTools/usr/bin/make release-programs\n",
      "for subdir in misc dstruct lm flm lattice utils zlib; do \\\n",
      "\t\t(cd $subdir/src; /Library/Developer/CommandLineTools/usr/bin/make SRILM=/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm MACHINE_TYPE=macosx OPTION= MAKE_PIC= release-programs) || exit 1; \\\n",
      "\tdone\n",
      "make[2]: Nothing to be done for `release-programs'.\n",
      "make[2]: Nothing to be done for `release-programs'.\n",
      "make[2]: Nothing to be done for `release-programs'.\n",
      "make[2]: Nothing to be done for `release-programs'.\n",
      "make[2]: Nothing to be done for `release-programs'.\n",
      "make[2]: Nothing to be done for `release-programs'.\n",
      "make[2]: Nothing to be done for `release-programs'.\n",
      "/Library/Developer/CommandLineTools/usr/bin/make release-scripts\n",
      "for subdir in misc dstruct lm flm lattice utils zlib; do \\\n",
      "\t\t(cd $subdir/src; /Library/Developer/CommandLineTools/usr/bin/make SRILM=/Users/marcellmaitinsky/LING242/computational-tools-for-linguistic-analysis-ubc/labs/srilm MACHINE_TYPE=macosx OPTION= MAKE_PIC= release-scripts) || exit 1; \\\n",
      "\tdone\n",
      "make[2]: Nothing to be done for `release-scripts'.\n",
      "make[2]: Nothing to be done for `release-scripts'.\n",
      "make[2]: Nothing to be done for `release-scripts'.\n",
      "make[2]: Nothing to be done for `release-scripts'.\n",
      "make[2]: Nothing to be done for `release-scripts'.\n",
      "make[2]: Nothing to be done for `release-scripts'.\n",
      "make[2]: Nothing to be done for `release-scripts'.\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c6aa7-e0af-44e2-b337-f535c9b14746",
   "metadata": {},
   "source": [
    "##### List all files with name ngram-count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5155a6ae-bec2-45de-9f0a-36e2489116b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-r-xr-xr-x  1 marcellmaitinsky  staff  1699400 22 sep 17:39 \u001b[31mbin/macosx/ngram-count\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -l bin/macosx/ngram-count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48165f48-bbad-46c9-81c3-7ad2aae6d8d7",
   "metadata": {},
   "source": [
    "##### Estimate n-gram probabilities. First name the text file you are using as training data, then the file that you are writing the language model to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04867c97-c8e9-4503-99a5-d142c972818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: discount coeff 1 is out of range: 0\n"
     ]
    }
   ],
   "source": [
    "!./bin/macosx/ngram-count -text ./simple-examples/data/ptb.train.txt -lm text.lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d703ac4-f493-46b7-9018-29a103c994a9",
   "metadata": {},
   "source": [
    "##### The above warning can be ignored for the purposes of this assignment, and does not affect the final step. All it tells us is that the dataset we are working with is very small. This step had no required output, so if you see nothing pop up other than this warning, that means the estimating step has been successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893032d6-cb9e-4e9a-88cd-f886e827032a",
   "metadata": {},
   "source": [
    "##### Complete querying/evaluation for your text.lm language model. This time the language model goes first, followed by the data that you are comparing to find the perplexity of your model — indeed, \"ppl\" is short for perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48ecbb02-5985-40ca-a8d2-c7f35e9d323f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ./simple-examples/data/ptb.test.txt: 3761 sentences, 78669 words, 4794 OOVs\n",
      "0 zeroprobs, logprob= -176327.4 ppl= 186.727 ppl1= 243.6885\n"
     ]
    }
   ],
   "source": [
    "!./bin/macosx/ngram -lm text.lm -ppl ./simple-examples/data/ptb.test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c8949-ee09-4ce5-9b56-431e08bdca5e",
   "metadata": {},
   "source": [
    "##### The output above shows the results of querying, including basic data on number of sentences, words, and OOVs, followed by the log probability (see KenLM section for explanation), perplexity, and perplexity with add-one smoothing (\"ppl1\" because each probability is incremented by 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f70dc-95f4-457a-9976-0611d90dc189",
   "metadata": {},
   "source": [
    "##### Because the KenLM and SRILM sections of this lab worked with slightly different data, it is not possible to determine from this demonstration which of the two is better for which purpose, but things like perplexity could in theory be compared if the data for the language model was exactly the same. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
