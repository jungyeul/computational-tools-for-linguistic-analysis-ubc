{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LING 242 Python Lecture 7: Text processing\n",
    "\n",
    "* Sentence segmentation\n",
    "* Tokenization\n",
    "* Lemmatization and Stemming\n",
    "* POS tagging\n",
    "* End-to-end preprocessing with SpaCy\n",
    "* T/F questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For many computational linguistics applications, the sentence is a key unit of processing. All modern written languages have an end-of-sentence marker. For some languages, like Chinese, this marker is unambigious and so getting the sentences of the text is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你好', '我叫J', '我是老师', '']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_text = \"你好。我叫J。我是老师。\"\n",
    "\n",
    "zh_sents = zh_text.split(\"。\")\n",
    "zh_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, English is not so easy because the period (\".\") is ambiguous. It has various uses. This is true for many other European languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The luxury auto maker last year sold 1,214 cars in the U\n",
      "S\n",
      " Howard Mosher, president and chief executive officer, said he anticipates growth for the luxury auto maker in Britain and Europe, and in Far Eastern markets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_text = \"The luxury auto maker last year sold 1,214 cars in the U.S. Howard Mosher, president and chief executive officer, said he anticipates growth for the luxury auto maker in Britain and Europe, and in Far Eastern markets.\"\n",
    "\n",
    "for sent in en_text.split(\".\"):\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are ways to improve things considerably. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For major languages with significant ambiguity, you'll probably want to use a dedicated sentence splitter, which NLTK has for some languages. But don't expect perfection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The luxury auto maker last year sold 1,214 cars in the U.S. Howard Mosher, president and chief executive officer, said he anticipates growth for the luxury auto maker in Britain and Europe, and in Far Eastern markets.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sent_tokenize(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The luxury auto maker last year sold 1,214 cars in the U.S.',\n",
       " 'Howard Mosher, president and chief executive officer, said he anticipates growth for the luxury auto maker in Britain and Europe, and in Far Eastern markets.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "\n",
    "tokenizer.tokenize(en_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, breaking a sentence up into words seems like it should be easy, but actually rarely is. Spaces separate most word tokens, but punctuation is a problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Howard Mosher, president and chief executive officer, said he anticipates growth for the luxury auto maker in Britain and Europe, and in Far Eastern markets.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = tokenizer.tokenize(en_text)\n",
    "sent = sents[1]\n",
    "sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Howard',\n",
       " 'Mosher,',\n",
       " 'president',\n",
       " 'and',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'officer,',\n",
       " 'said',\n",
       " 'he',\n",
       " 'anticipates',\n",
       " 'growth',\n",
       " 'for',\n",
       " 'the',\n",
       " 'luxury',\n",
       " 'auto',\n",
       " 'maker',\n",
       " 'in',\n",
       " 'Britain',\n",
       " 'and',\n",
       " 'Europe,',\n",
       " 'and',\n",
       " 'in',\n",
       " 'Far',\n",
       " 'Eastern',\n",
       " 'markets.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Howard',\n",
       " 'Mosher',\n",
       " 'president',\n",
       " 'and',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'officer',\n",
       " 'said',\n",
       " 'he',\n",
       " 'anticipates',\n",
       " 'growth',\n",
       " 'for',\n",
       " 'the',\n",
       " 'luxury',\n",
       " 'auto',\n",
       " 'maker',\n",
       " 'in',\n",
       " 'Britain',\n",
       " 'and',\n",
       " 'Europe',\n",
       " 'and',\n",
       " 'in',\n",
       " 'Far',\n",
       " 'Eastern',\n",
       " 'markets']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "[match.group() for match in re.finditer(\"\\w+\", sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In computational linguistics for English, *clitics* (small words that are phonologically joined to a host word) such as \"n't\" and \"'s\" are often treated as separate words, and need to be dealt with specially). A proper word tokenizer (such as is included in NLTK) will deal with these subtle issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Howard',\n",
       " 'Mosher',\n",
       " ',',\n",
       " 'president',\n",
       " 'and',\n",
       " 'chief',\n",
       " 'executive',\n",
       " 'officer',\n",
       " ',',\n",
       " 'said',\n",
       " 'he',\n",
       " 'anticipates',\n",
       " 'growth',\n",
       " 'for',\n",
       " 'the',\n",
       " 'luxury',\n",
       " 'auto',\n",
       " 'maker',\n",
       " 'in',\n",
       " 'Britain',\n",
       " 'and',\n",
       " 'Europe',\n",
       " ',',\n",
       " 'and',\n",
       " 'in',\n",
       " 'Far',\n",
       " 'Eastern',\n",
       " 'markets',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In some languages (such as Chinese) there are no spaces in the words. Much more sophisticated word segmenters are needed in this case, for example [jieba](https://github.com/fxsjy/jieba) for Chinese (you'll need to install the package to run this code). One challenge for these languages is that it isn't always clear what a word is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/n9/3r6hrp015t9d1n6m8l36t0yh0000gn/T/jieba.cache\n",
      "Loading model cost 0.301 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词 是 小菜一碟\n"
     ]
    }
   ],
   "source": [
    "# !pip install jieba\n",
    "from jieba import cut\n",
    "zh_text = \"分词是小菜一碟\" # tokenization is a piece of cake\n",
    "\n",
    "print(\" \".join(cut(zh_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For some applications, it can be useful to ignore the morphological differences between words. A classic example is information retrival (i.e. web search); if a user looks for \"sleeping kitties\", you might want to include in your results a page which mentions that \"the kitty sleeps\". By default, though, \"kitty\" and \"kitties\" and \"sleeps\" and \"sleeping\" are completely different word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = [\"The\", \"kitty\", \"sleeps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"sleeping\" in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"kitties\" in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lemmatization converts an inflected form to a base, uninflected form. This decreases the size of your vocabulary (eliminating rare forms), and is often useful if you want calculate statistics using lexicons but don't want to store all the possible forms.  NLTK has a lemmatizer for English that uses the WordNet lexicon (more on WordNet in COLX 561). One tricky aspect is that by default it requires a part-of-speech. It works for both regular and irregular forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitty'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"kitties\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"sleeping\", \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"women\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"sleep\", \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want to POS tag, trying to lemmatize as a noun first, then a verb, will work in most cases, but not all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, \"n\")\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, \"v\")\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitty'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"kitties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doe'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"does\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"does\", \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stemming has a similar purpose but it strips off both inflectional and derivational mophology to reach a stem. This stem is not always itself a word. Sometime stemming incorrectly collapses/conflates words with very different meaning, or fails to collapse related words. The most popular stemming algorithm for English is called the [porter stemmer](http://snowball.tartarus.org/algorithms/porter/stemmer.html), it involves a bunch of rewrite rules to get rid of common suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitti'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"kitty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitti'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"kitties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = \"automatization\"\n",
    "S2 = \"automatic\"\n",
    "S3 = \"has\"\n",
    "S4 = \"have\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automat'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automat'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ha'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's try both lemmatizing and stemming the sentence below (after tokenizing), and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whereas', 'disregard', 'and', 'contempt', 'for', 'human', 'right', 'have', 'result', 'in', 'barbarous', 'act', 'which', 'have', 'outrage', 'the', 'conscience', 'of', 'mankind', ',', 'and', 'the', 'advent', 'of', 'a', 'world', 'in', 'which', 'human', 'being', 'shall', 'enjoy', 'freedom', 'of', 'speech', 'and', 'belief', 'and', 'freedom', 'from', 'fear', 'and', 'want', 'ha', 'be', 'proclaim', 'a', 'the', 'highest', 'aspiration', 'of', 'the', 'common', 'people']\n",
      "['wherea', 'disregard', 'and', 'contempt', 'for', 'human', 'right', 'have', 'result', 'in', 'barbar', 'act', 'which', 'have', 'outrag', 'the', 'conscienc', 'of', 'mankind', ',', 'and', 'the', 'advent', 'of', 'a', 'world', 'in', 'which', 'human', 'be', 'shall', 'enjoy', 'freedom', 'of', 'speech', 'and', 'belief', 'and', 'freedom', 'from', 'fear', 'and', 'want', 'ha', 'been', 'proclaim', 'as', 'the', 'highest', 'aspir', 'of', 'the', 'common', 'peopl']\n"
     ]
    }
   ],
   "source": [
    "S = \"Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people\"\n",
    "words = word_tokenize(S)\n",
    "lemmas = [lemmatize(word) for word in words]\n",
    "print(lemmas)\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've already seen that POS tagging can be useful for doing analysis of corpora. It has other uses, for instance it can be used to focus on particular kinds of words for certain applications, and it can provide simple word sense disambiguation (e.g. the word \"cross\"). The NLTK POS tagger for English is easy to use and effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('think', 'VBP'), ('the', 'DT'), ('NLTK', 'NNP'), ('pos', 'NN'), ('tagger', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('pretty', 'RB'), ('solid', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('doing', 'VBG'), ('computational', 'JJ'), ('linguistics', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "S = \"I think the NLTK pos tagger is a pretty solid tool for doing computational linguistics\"\n",
    "print(pos_tag(S.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The standard tagset for English and used for the NLTK tagger (and most others) is the one created as part of the Penn Treebank annotation project, defined [here](https://www.anc.org/penn.html). We can also POS tag using the universal tagset if we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/jungyeul/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " ('think', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('NLTK', 'NOUN'),\n",
       " ('pos', 'NOUN'),\n",
       " ('tagger', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('pretty', 'ADV'),\n",
       " ('solid', 'ADJ'),\n",
       " ('tool', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('doing', 'VERB'),\n",
       " ('computational', 'ADJ'),\n",
       " ('linguistics', 'NOUN')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(S.split(\" \"), tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also use NLTK to build a POS tagger for any language where we have a manually tagged corpus and/or morphological information which can be expressed in the form of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 'PRP'), ('googled', 'VBD'), ('cats', 'NNS')]\n",
      "[('he', 'PRP'), ('googled', 'VBD'), ('cats', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTagger, UnigramTagger\n",
    "from nltk.corpus import treebank \n",
    "\n",
    "patterns = [(r\"\\d+\", \"CD\"),(r\".*ing$\", \"VBG\"), (r\".*ed$\", \"VBD\"),(r\".*s$\", \"NNS\"),(r\".*\", \"NN\")]\n",
    "sentence = [\"he\", \"googled\", \"cats\"]\n",
    "tagged = treebank.tagged_sents()\n",
    "\n",
    "re_tagger= RegexpTagger(patterns)\n",
    "uni_tagger= UnigramTagger(tagged, backoff=re_tagger)\n",
    "print(pos_tag(sentence))\n",
    "print(uni_tagger.tag(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use NLTK to tokenize the sentences about me, and then, for each sentence, compare the tags from the main NLTK POS tagger and the simple one we just built. Are there many differences? When there are, which do you think is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "DT\n",
      "DT\n",
      "luxury\n",
      "NN\n",
      "NN\n",
      "auto\n",
      "NN\n",
      "NN\n",
      "maker\n",
      "NN\n",
      "NN\n",
      "last\n",
      "JJ\n",
      "JJ\n",
      "year\n",
      "NN\n",
      "NN\n",
      "sold\n",
      "VBD\n",
      "VBN\n",
      "1,214\n",
      "CD\n",
      "CD\n",
      "cars\n",
      "NNS\n",
      "NNS\n",
      "in\n",
      "IN\n",
      "IN\n",
      "the\n",
      "DT\n",
      "DT\n",
      "U.S\n",
      "NNP\n",
      "NN\n",
      ".\n",
      ".\n",
      ".\n",
      "Howard\n",
      "NNP\n",
      "NNP\n",
      "Mosher\n",
      "NNP\n",
      "NN\n",
      ",\n",
      ",\n",
      ",\n",
      "president\n",
      "NN\n",
      "NN\n",
      "and\n",
      "CC\n",
      "CC\n",
      "chief\n",
      "JJ\n",
      "NN\n",
      "executive\n",
      "NN\n",
      "NN\n",
      "officer\n",
      "NN\n",
      "NN\n",
      ",\n",
      ",\n",
      ",\n",
      "said\n",
      "VBD\n",
      "VBD\n",
      "he\n",
      "PRP\n",
      "PRP\n",
      "anticipates\n",
      "VBZ\n",
      "VBZ\n",
      "growth\n",
      "NN\n",
      "NN\n",
      "for\n",
      "IN\n",
      "IN\n",
      "the\n",
      "DT\n",
      "DT\n",
      "luxury\n",
      "NN\n",
      "NN\n",
      "auto\n",
      "NN\n",
      "NN\n",
      "maker\n",
      "NN\n",
      "NN\n",
      "in\n",
      "IN\n",
      "IN\n",
      "Britain\n",
      "NNP\n",
      "NNP\n",
      "and\n",
      "CC\n",
      "CC\n",
      "Europe\n",
      "NNP\n",
      "NNP\n",
      ",\n",
      ",\n",
      ",\n",
      "and\n",
      "CC\n",
      "CC\n",
      "in\n",
      "IN\n",
      "IN\n",
      "Far\n",
      "NNP\n",
      "RB\n",
      "Eastern\n",
      "NNP\n",
      "NNP\n",
      "markets\n",
      "NNS\n",
      "NNS\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    # my code here\n",
    "    tokens = word_tokenize(sent)\n",
    "    NLTK_pos = pos_tag(tokens)\n",
    "    my_pos = uni_tagger.tag(tokens)\n",
    "    for i in range(len(tokens)):\n",
    "        print(tokens[i])\n",
    "        print(NLTK_pos[i][1])\n",
    "        print(my_pos[i][1])\n",
    "    # my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All-in-one preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLTK isn't the only option for preprocessing in English with Python. Another popular choice is [SpaCy](https://spacy.io), which will do everything you might need in one line of code (including stuff we aren't talking about in this course, i.e. parsing). You'll need to install the package and its models. It's fast and lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Hi there. How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can access what you need in the resulting document object. The sentences are in sents, each sentence is a list of tokens, and each token has a lemmas_, pos_ (the universal POS tag), and tag_ attribute (the treebank pos tag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there.\n",
      "token: Hi\n",
      "lemma: hi\n",
      "Univeral POS: INTJ\n",
      "PTB POS: UH\n",
      "-----\n",
      "token: there\n",
      "lemma: there\n",
      "Univeral POS: ADV\n",
      "PTB POS: RB\n",
      "-----\n",
      "token: .\n",
      "lemma: .\n",
      "Univeral POS: PUNCT\n",
      "PTB POS: .\n",
      "-----\n",
      "How are you?\n",
      "token: How\n",
      "lemma: how\n",
      "Univeral POS: SCONJ\n",
      "PTB POS: WRB\n",
      "-----\n",
      "token: are\n",
      "lemma: be\n",
      "Univeral POS: AUX\n",
      "PTB POS: VBP\n",
      "-----\n",
      "token: you\n",
      "lemma: you\n",
      "Univeral POS: PRON\n",
      "PTB POS: PRP\n",
      "-----\n",
      "token: ?\n",
      "lemma: ?\n",
      "Univeral POS: PUNCT\n",
      "PTB POS: .\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        print(\"token:\", token)\n",
    "        print(\"lemma:\", token.lemma_)\n",
    "        print(\"Univeral POS:\", token.pos_)  \n",
    "        print(\"PTB POS:\", token.tag_)\n",
    "        print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at the results for SpaCy and NLTK for preprocessing the sentence about me and find some difference in the results of their preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "[('The', 'DT'), ('luxury', 'NN'), ('auto', 'NN'), ('maker', 'NN'), ('last', 'JJ'), ('year', 'NN'), ('sold', 'VBD'), ('1,214', 'CD'), ('cars', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('U.S.', 'NNP'), ('Howard', 'NNP'), ('Mosher', 'NNP'), (',', ','), ('president', 'NN'), ('and', 'CC'), ('chief', 'JJ'), ('executive', 'JJ'), ('officer', 'NN'), (',', ','), ('said', 'VBD'), ('he', 'PRP'), ('anticipates', 'VBZ'), ('growth', 'NN'), ('for', 'IN'), ('the', 'DT'), ('luxury', 'NN'), ('auto', 'NN'), ('maker', 'NN'), ('in', 'IN'), ('Britain', 'NNP'), ('and', 'CC'), ('Europe', 'NNP'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('Far', 'JJ'), ('Eastern', 'JJ'), ('markets', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('luxury', 'NN'), ('auto', 'NN'), ('maker', 'NN'), ('last', 'JJ'), ('year', 'NN'), ('sold', 'VBD'), ('1,214', 'CD'), ('cars', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('U.S.', 'NNP'), ('Howard', 'NNP'), ('Mosher', 'NNP'), (',', ','), ('president', 'NN'), ('and', 'CC'), ('chief', 'JJ'), ('executive', 'NN'), ('officer', 'NN'), (',', ','), ('said', 'VBD'), ('he', 'PRP'), ('anticipates', 'VBZ'), ('growth', 'NN'), ('for', 'IN'), ('the', 'DT'), ('luxury', 'NN'), ('auto', 'NN'), ('maker', 'NN'), ('in', 'IN'), ('Britain', 'NNP'), ('and', 'CC'), ('Europe', 'NNP'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('Far', 'NNP'), ('Eastern', 'NNP'), ('markets', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "spacy_sents = []\n",
    "nltk_sents = []\n",
    "\n",
    "en_doc = nlp(en_text)\n",
    "for sent in en_doc.sents:\n",
    "    spacy_sents.append([(str(token), token.tag_) for token in sent])\n",
    "for sent in sent_tokenize(en_text):\n",
    "    nltk_sents.append(pos_tag(word_tokenize(sent)))\n",
    "\n",
    "print(len(spacy_sents))\n",
    "print(len(nltk_sents))\n",
    "for i in range(len(spacy_sents)):\n",
    "    print(spacy_sents[i])\n",
    "    print(nltk_sents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SpaCy also has [multilingual support](https://spacy.io/usage/models#languages). Let's take a quick look at French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considérant que la méconnaissance et le mépris des droits de l'homme ont conduit à des actes de barbarie qui révoltent la conscience de l'humanité et que l'avènement d'un monde où les êtres humains seront libres de parler et de croire, libérés de la terreur et de la misère, a été proclamé comme la plus haute aspiration de l'homme\n",
      "token: Considérant\n",
      "lemma: considérer\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: que\n",
      "lemma: que\n",
      "Universal POS: SCONJ\n",
      "Language-specific POS: SCONJ\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: méconnaissance\n",
      "lemma: méconnaissance\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: le\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: mépris\n",
      "lemma: mépris\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: des\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: droits\n",
      "lemma: droit\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: homme\n",
      "lemma: homme\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: ont\n",
      "lemma: avoir\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: conduit\n",
      "lemma: conduire\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: à\n",
      "lemma: à\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: des\n",
      "lemma: un\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: actes\n",
      "lemma: acte\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: barbarie\n",
      "lemma: barbarie\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: qui\n",
      "lemma: qui\n",
      "Universal POS: PRON\n",
      "Language-specific POS: PRON\n",
      "------\n",
      "token: révoltent\n",
      "lemma: révolter\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: conscience\n",
      "lemma: conscience\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: humanité\n",
      "lemma: humanité\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: que\n",
      "lemma: que\n",
      "Universal POS: SCONJ\n",
      "Language-specific POS: SCONJ\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: avènement\n",
      "lemma: avènement\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: d'\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: un\n",
      "lemma: un\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: monde\n",
      "lemma: monde\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: où\n",
      "lemma: où\n",
      "Universal POS: PRON\n",
      "Language-specific POS: PRON\n",
      "------\n",
      "token: les\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: êtres\n",
      "lemma: être\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: humains\n",
      "lemma: humain\n",
      "Universal POS: ADJ\n",
      "Language-specific POS: ADJ\n",
      "------\n",
      "token: seront\n",
      "lemma: être\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: libres\n",
      "lemma: libre\n",
      "Universal POS: ADJ\n",
      "Language-specific POS: ADJ\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: parler\n",
      "lemma: parler\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: croire\n",
      "lemma: croire\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: ,\n",
      "lemma: ,\n",
      "Universal POS: PUNCT\n",
      "Language-specific POS: PUNCT\n",
      "------\n",
      "token: libérés\n",
      "lemma: libérer\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: terreur\n",
      "lemma: terreur\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: misère\n",
      "lemma: misère\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: ,\n",
      "lemma: ,\n",
      "Universal POS: PUNCT\n",
      "Language-specific POS: PUNCT\n",
      "------\n",
      "token: a\n",
      "lemma: avoir\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: été\n",
      "lemma: être\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: proclamé\n",
      "lemma: proclamer\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: comme\n",
      "lemma: comme\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: plus\n",
      "lemma: plus\n",
      "Universal POS: ADV\n",
      "Language-specific POS: ADV\n",
      "------\n",
      "token: haute\n",
      "lemma: haute\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: aspiration\n",
      "lemma: aspiration\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: homme\n",
      "lemma: homme\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m spacy download fr_core_news_sm\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\"Considérant que la méconnaissance et le mépris des droits de l'homme ont conduit à des actes de barbarie qui révoltent la conscience de l'humanité et que l'avènement d'un monde où les êtres humains seront libres de parler et de croire, libérés de la terreur et de la misère, a été proclamé comme la plus haute aspiration de l'homme\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        print(\"token:\", token)\n",
    "        print(\"lemma:\", token.lemma_)\n",
    "        print(\"Universal POS:\", token.pos_)  \n",
    "        print(\"Language-specific POS:\", token.tag_)\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SpaCy is solid and stable, but if you want the very best, state-of-the-art NLP tools and are willing to sacrifice speed, you might also try [Flair](https://github.com/zalandoresearch/flair). [Textblob](https://textblob.readthedocs.io/en/dev/) is another popular one-stop option for NLP processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T/F questions\n",
    "\n",
    "1. The only reason English sentence segmentation is challenging is that the period is ambiguous.\n",
    "2. Word tokenization for Chinese is easier than English, because Chinese characters correspond directly to words, whereas English has clitics.\n",
    "3. After you stem a word, what is left is also a word.\n",
    "4. The morphology of a word (its affixes) gives you a lot of information about what its part of speech is.\n",
    "5. If you are doing both lemmatization and POS tagging, you should do POS tagging first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [_NLTK BOOK_](https://www.nltk.org/book/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
